{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AgentX Documentation","text":"<p>Welcome to the AgentX documentation. AgentX is an AI Agent Platform combining MCP client integration, drafting models, reasoning frameworks, and a sophisticated memory system.</p>"},{"location":"#overview","title":"Overview","text":"<p>AgentX is built on a modern, two-tier architecture:</p> <ul> <li>Backend: Django REST API providing AI-powered services</li> <li>Frontend: Tauri desktop application with React/TypeScript</li> <li>AI Features: Multi-level translation, MCP client, drafting models, reasoning framework</li> <li>Memory Stack: Neo4j graph database, PostgreSQL with pgvector, and Redis</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#translation-system","title":"Translation System","text":"<p>Multi-level language detection and translation supporting 200+ languages:</p> <ul> <li>Fast initial detection (~20 languages)</li> <li>Comprehensive translation using NLLB-200 architecture</li> <li>Confidence scoring and fallback mechanisms</li> </ul>"},{"location":"#memory-system","title":"Memory System","text":"<p>Hybrid memory architecture combining:</p> <ul> <li>Neo4j: Graph-based relationship analysis</li> <li>PostgreSQL + pgvector: Vector embeddings for semantic search</li> <li>Redis: Fast in-memory caching</li> </ul>"},{"location":"#desktop-application","title":"Desktop Application","text":"<p>Native desktop experience with:</p> <ul> <li>Tab-based interface (Dashboard, Translation, Chat, Tools)</li> <li>Cross-platform support via Tauri v2</li> <li>Fast development with Vite + React 19</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p>:material-rocket-launch:{ .lg .middle } Getting Started</p> <p>Install and run AgentX in minutes</p> <p>:octicons-arrow-right-24: Installation</p> </li> <li> <p>:material-code-braces:{ .lg .middle } Development</p> <p>Set up your development environment</p> <p>:octicons-arrow-right-24: Setup Guide</p> </li> <li> <p>:material-api:{ .lg .middle } API Reference</p> <p>Explore the REST API endpoints</p> <p>:octicons-arrow-right-24: API Docs</p> </li> <li> <p>:material-database:{ .lg .middle } Database Stack</p> <p>Learn about the database architecture</p> <p>:octicons-arrow-right-24: Databases</p> </li> <li> <p>:material-road-variant:{ .lg .middle } Roadmap</p> <p>Development history and future plans</p> <p>:octicons-arrow-right-24: Roadmap</p> </li> </ul>"},{"location":"#architecture-at-a-glance","title":"Architecture at a Glance","text":"<pre><code>graph TB\n    Client[Tauri Client&lt;br/&gt;React + TypeScript]\n    API[Django API&lt;br/&gt;Port 12319]\n    Neo4j[Neo4j&lt;br/&gt;Graph DB]\n    Postgres[PostgreSQL&lt;br/&gt;+ pgvector]\n    Redis[Redis&lt;br/&gt;Cache]\n\n    Client --&gt;|HTTP| API\n    API --&gt; Neo4j\n    API --&gt; Postgres\n    API --&gt; Redis</code></pre>"},{"location":"#technology-stack","title":"Technology Stack","text":"Layer Technology Purpose Frontend Tauri v2 + React 19 Desktop application shell Build Vite + TypeScript Fast development &amp; bundling Backend Django 5.2.8 REST API framework AI/ML HuggingFace Transformers Language models Graph DB Neo4j 5.15 Relationship analysis Vector DB PostgreSQL + pgvector Semantic search Cache Redis 7 In-memory data store Task Runner Task (Taskfile) Development automation Package Manager uv Fast Python dependency management"},{"location":"#project-status","title":"Project Status","text":"<p>AgentX is under active development. See the Development Roadmap for detailed progress.</p> <p>Completed (Phases 1-10): - Django API with translation endpoints - Tauri desktop application with cosmic theme - Two-level translation system (200+ languages) - Database stack (Neo4j, Postgres, Redis) - MCP client integration - Model provider abstraction (OpenAI, Anthropic, Ollama) - Drafting framework (speculative, pipeline, candidates) - Reasoning framework (CoT, ToT, ReAct, Reflection) - Agent core with task planning - Core test suite (50 tests)</p> <p>In Progress: - Phase 11: Memory System (90%) - Phase 13: UI Implementation (15%)</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"roadmap/","title":"AgentX Development Roadmap","text":"<p>This document tracks the development history and future direction of AgentX.</p>"},{"location":"roadmap/#progress-overview","title":"Progress Overview","text":"Phase Status Description Phase 1: Critical Fixes Complete Codebase baseline and dependency fixes Phase 2: Wire Up Code Complete Connect scaffolded components Phase 3: MCP Client Complete External tool server integration Phase 4: Model Providers Complete Multi-provider LLM abstraction Phase 5: Drafting Framework Complete Speculative decoding and pipelines Phase 6: Reasoning Framework Complete CoT, ToT, ReAct, Reflection Phase 7: Agent Core Complete Unified agent architecture Phase 8: Client Updates Complete Tauri UI with cosmic theme Phase 9: Security Complete Foundation security measures Phase 10: Testing (Core) Complete 50 tests covering core functionality Phase 11: Memory System In Progress Persistent knowledge graphs Phase 12: Documentation Planned Comprehensive docs Phase 13: UI Implementation In Progress Chat and Agent interfaces"},{"location":"roadmap/#completed-phases","title":"Completed Phases","text":""},{"location":"roadmap/#phase-1-critical-fixes","title":"Phase 1: Critical Fixes","text":"<p>Goal: Get the codebase into a working baseline state</p>"},{"location":"roadmap/#11-missing-dependencies","title":"1.1 Missing Dependencies","text":"<ul> <li>Added <code>redis&gt;=5.0.0</code>, <code>sqlalchemy&gt;=2.0.0</code>, <code>mcp&gt;=1.0.0</code> to pyproject.toml</li> <li>Added <code>openai&gt;=1.0.0</code>, <code>anthropic&gt;=0.20.0</code>, <code>httpx&gt;=0.27.0</code> for model providers</li> <li>Added <code>networkx&gt;=3.0</code> for reasoning graphs</li> <li>Ran <code>uv sync</code> to regenerate lock file and verified imports</li> </ul>"},{"location":"roadmap/#12-taskfile-corrections","title":"1.2 Taskfile Corrections","text":"<ul> <li>Fixed <code>client:build</code>: <code>bunx next build</code> \u2192 <code>bun run build</code></li> <li>Fixed <code>client:start</code>: <code>bunx next start</code> \u2192 <code>bun run preview</code></li> <li>Fixed <code>client:dev</code>: <code>bunx tauri run dev</code> \u2192 <code>bun run tauri dev</code></li> <li>Removed Next.js references (project uses Vite)</li> </ul>"},{"location":"roadmap/#13-openapi-specification","title":"1.3 OpenAPI Specification","text":"<ul> <li>Updated server URL port: <code>8000</code> \u2192 <code>12319</code></li> <li>Documented <code>POST /tools/translate</code> endpoint with request/response schemas</li> <li>Fixed language-detect path: <code>/language-detect</code> \u2192 <code>/tools/language-detect-20</code></li> <li>Added proper response schemas and Error component schema</li> </ul>"},{"location":"roadmap/#14-environment-configuration","title":"1.4 Environment Configuration","text":"<ul> <li>Created <code>.env.example</code> with all required variables</li> <li>Updated <code>docker-compose.yml</code> to use environment variables</li> <li>Ensured <code>.env</code> is in <code>.gitignore</code></li> <li>Added redis-commander to optional <code>tools</code> profile</li> </ul>"},{"location":"roadmap/#15-code-quality","title":"1.5 Code Quality","text":"<ul> <li>Replaced <code>print()</code> statements with <code>logging</code> in views.py</li> <li>Fixed <code>language_detect</code> to accept POST with text body (backwards compatible)</li> <li>Fixed test import paths and URL paths to match new API structure</li> <li>Updated CLAUDE.md with correct API endpoints</li> </ul>"},{"location":"roadmap/#phase-2-wire-up-existing-code","title":"Phase 2: Wire Up Existing Code","text":"<p>Goal: Connect the scaffolded code that already exists</p>"},{"location":"roadmap/#21-language-detection-api-fix","title":"2.1 Language Detection API Fix","text":"<ul> <li>Modified <code>views.language_detect()</code> to accept POST</li> <li>Removed hardcoded test string</li> <li>Added input validation with structured response (language code + confidence)</li> </ul>"},{"location":"roadmap/#22-agent-memory-system-connection","title":"2.2 Agent Memory System Connection","text":"<ul> <li>Renamed <code>agent_memory.py</code> \u2192 <code>memory_utils.py</code> to avoid naming conflict</li> <li>Added database connection health check endpoint: <code>GET /api/health</code></li> <li>Created lazy-loading <code>get_agent_memory()</code> function</li> <li>Made PostgreSQL connection lazy (PostgresConnection class)</li> <li>Added <code>check_memory_health()</code> for connection status</li> </ul>"},{"location":"roadmap/#23-code-quality-improvements","title":"2.3 Code Quality Improvements","text":"<ul> <li>Replaced all <code>print()</code> statements with <code>logging</code></li> <li>Added logging configuration to Django settings</li> <li>Added <code>psycopg2-binary</code> for PostgreSQL connectivity</li> <li>Added <code>sentence-transformers</code> for local embeddings</li> <li>Added health check tests</li> </ul>"},{"location":"roadmap/#phase-3-mcp-client-integration","title":"Phase 3: MCP Client Integration","text":"<p>Goal: Enable AgentX to consume tools from external MCP servers</p>"},{"location":"roadmap/#31-mcp-client-infrastructure","title":"3.1 MCP Client Infrastructure","text":"<p>Created module structure: <pre><code>api/agentx_ai/mcp/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 client.py           # MCP client manager\n\u251c\u2500\u2500 server_registry.py  # Track connected MCP servers\n\u251c\u2500\u2500 tool_executor.py    # Execute tools on remote servers\n\u2514\u2500\u2500 transports/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 stdio.py        # stdio transport (subprocess)\n    \u2514\u2500\u2500 sse.py          # SSE transport (HTTP)\n</code></pre></p> <p>Implemented <code>MCPClientManager</code> class: - <code>connect_server(server_config)</code> \u2192 async context manager with ServerConnection - <code>list_tools(server_name?)</code> \u2192 available tools - <code>call_tool(server_name, tool_name, args)</code> \u2192 ToolResult - <code>list_resources(server_name?)</code> \u2192 available resources - <code>read_resource(server_name, uri)</code> \u2192 content</p>"},{"location":"roadmap/#32-server-configuration","title":"3.2 Server Configuration","text":"<ul> <li>Created <code>mcp_servers.json.example</code> configuration format</li> <li>Added Taskfile commands: <code>task mcp:list-servers</code>, <code>task mcp:list-tools</code></li> <li>Added API endpoints: <code>/api/mcp/servers</code>, <code>/api/mcp/tools</code>, <code>/api/mcp/resources</code></li> </ul>"},{"location":"roadmap/#33-tool-discovery-caching","title":"3.3 Tool Discovery &amp; Caching","text":"<ul> <li>Cache tool schemas on connection (in ToolExecutor)</li> <li>Handle server disconnection gracefully (via context managers)</li> </ul>"},{"location":"roadmap/#phase-4-model-provider-abstraction","title":"Phase 4: Model Provider Abstraction","text":"<p>Goal: Support multiple LLM backends with unified interface</p>"},{"location":"roadmap/#41-provider-interface","title":"4.1 Provider Interface","text":"<p>Created abstract <code>ModelProvider</code> interface: <pre><code>class ModelProvider(ABC):\n    @abstractmethod\n    async def complete(self, messages, **kwargs) -&gt; CompletionResult\n\n    @abstractmethod\n    async def stream(self, messages, **kwargs) -&gt; AsyncIterator[StreamChunk]\n\n    @abstractmethod\n    def get_capabilities(self) -&gt; ModelCapabilities\n</code></pre></p>"},{"location":"roadmap/#42-provider-implementations","title":"4.2 Provider Implementations","text":"<ul> <li><code>OpenAIProvider</code> - GPT-4, GPT-4-turbo, GPT-3.5</li> <li><code>AnthropicProvider</code> - Claude 3 Opus/Sonnet/Haiku</li> <li><code>OllamaProvider</code> - Local models (Llama, Mistral, etc.)</li> </ul>"},{"location":"roadmap/#43-model-registry","title":"4.3 Model Registry","text":"<ul> <li>Created <code>models.yaml</code> configuration</li> <li>Model selection based on task requirements (via ProviderRegistry)</li> </ul>"},{"location":"roadmap/#phase-5-drafting-models-framework","title":"Phase 5: Drafting Models Framework","text":"<p>Goal: Implement multi-model generation strategies</p>"},{"location":"roadmap/#51-drafting-infrastructure","title":"5.1 Drafting Infrastructure","text":"<p>Created module structure: <pre><code>api/agentx_ai/drafting/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 base.py            # DraftingStrategy, DraftResult\n\u251c\u2500\u2500 speculative.py     # Speculative decoding\n\u251c\u2500\u2500 pipeline.py        # Multi-model pipelines\n\u251c\u2500\u2500 candidate.py       # N-best candidate generation\n\u2514\u2500\u2500 drafting_strategies.yaml\n</code></pre></p>"},{"location":"roadmap/#52-speculative-decoding","title":"5.2 Speculative Decoding","text":"<p>Implemented <code>SpeculativeDecoder</code>: - Draft model generates N tokens quickly - Target model verifies/corrects - Configurable draft length and acceptance threshold - Support draft/target model pairs (configurable)</p>"},{"location":"roadmap/#53-multi-model-pipeline","title":"5.3 Multi-Model Pipeline","text":"<p>Implemented <code>ModelPipeline</code>: - Define pipeline stages (analyze \u2192 draft \u2192 refine \u2192 validate) - Route to different models per stage - Pass context between stages - Predefined stage roles (ANALYZE, DRAFT, REVIEW, REFINE, etc.)</p>"},{"location":"roadmap/#54-candidate-generation","title":"5.4 Candidate Generation","text":"<p>Implemented <code>CandidateGenerator</code>: - Generate N candidates (same or different models) - Scoring/ranking strategies:   - Self-consistency (majority vote)   - Verifier model scoring   - Heuristic scoring (length preference) - Best-of-N selection</p>"},{"location":"roadmap/#55-drafting-configuration","title":"5.5 Drafting Configuration","text":"<ul> <li>Created <code>drafting_strategies.yaml</code> with pre-configured strategies</li> </ul>"},{"location":"roadmap/#phase-6-reasoning-framework","title":"Phase 6: Reasoning Framework","text":"<p>Goal: Implement flexible reasoning patterns for complex tasks</p>"},{"location":"roadmap/#61-reasoning-infrastructure","title":"6.1 Reasoning Infrastructure","text":"<p>Created module structure: <pre><code>api/agentx_ai/reasoning/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 base.py              # ReasoningStrategy, ThoughtStep\n\u251c\u2500\u2500 chain_of_thought.py\n\u251c\u2500\u2500 tree_of_thought.py\n\u251c\u2500\u2500 react.py             # ReAct pattern\n\u251c\u2500\u2500 reflection.py        # Self-critique\n\u2514\u2500\u2500 orchestrator.py      # Combines strategies\n</code></pre></p>"},{"location":"roadmap/#62-chain-of-thought-cot","title":"6.2 Chain-of-Thought (CoT)","text":"<p>Implemented <code>ChainOfThought</code>: - Zero-shot CoT (\"Let's think step by step...\") - Few-shot CoT (with examples) - Auto-CoT mode - Step extraction and validation - Answer extraction</p>"},{"location":"roadmap/#63-tree-of-thought-tot","title":"6.3 Tree-of-Thought (ToT)","text":"<p>Implemented <code>TreeOfThought</code>: - BFS exploration (breadth-first) - DFS exploration (depth-first) - Beam search (top-k branches) - State evaluation heuristics - Pruning strategies - Tree serialization for traces</p>"},{"location":"roadmap/#64-react-reasoning-acting","title":"6.4 ReAct (Reasoning + Acting)","text":"<p>Implemented <code>ReActAgent</code>: - Thought \u2192 Action \u2192 Observation loop - Tool execution framework - Max iterations / termination conditions - Action parsing and execution - Built-in tools (search, calculate)</p>"},{"location":"roadmap/#65-reflection-self-critique","title":"6.5 Reflection &amp; Self-Critique","text":"<p>Implemented <code>ReflectiveReasoner</code>: - Generate initial response - Self-critique: identify weaknesses - Revise based on critique - Configurable reflection depth</p>"},{"location":"roadmap/#66-reasoning-orchestrator","title":"6.6 Reasoning Orchestrator","text":"<p>Implemented <code>ReasoningOrchestrator</code>: - Select reasoning strategy based on task - Task type classification - Fallback strategies on failure - Metrics: steps taken, tokens used, time elapsed</p>"},{"location":"roadmap/#phase-7-agent-core","title":"Phase 7: Agent Core","text":"<p>Goal: Unify MCP, drafting, and reasoning into coherent agent</p>"},{"location":"roadmap/#71-agent-architecture","title":"7.1 Agent Architecture","text":"<p>Created module structure: <pre><code>api/agentx_ai/agent/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 core.py            # Main Agent class\n\u251c\u2500\u2500 planner.py         # Task decomposition\n\u251c\u2500\u2500 context.py         # Context management\n\u2514\u2500\u2500 session.py         # Conversation session\n</code></pre></p>"},{"location":"roadmap/#72-agent-core-implementation","title":"7.2 Agent Core Implementation","text":"<p>Implemented <code>Agent</code> class with: - Integration with reasoning orchestrator - Integration with drafting strategies - Tool registration - Status tracking - Task cancellation</p>"},{"location":"roadmap/#73-task-planning","title":"7.3 Task Planning","text":"<p>Implemented <code>TaskPlanner</code>: - Decompose complex tasks into subtasks - Identify subtask types (research, analysis, generation, etc.) - Estimate complexity - Select reasoning strategy based on plan</p>"},{"location":"roadmap/#74-context-management","title":"7.4 Context Management","text":"<p>Implemented <code>ContextManager</code>: - Token estimation - Sliding window for long conversations - Summarization of old context - Memory injection support</p>"},{"location":"roadmap/#75-session-management","title":"7.5 Session Management","text":"<p>Implemented <code>Session</code> and <code>SessionManager</code>: - Message history tracking - Session creation/retrieval - Session cleanup for inactive sessions</p>"},{"location":"roadmap/#76-agent-api-endpoints","title":"7.6 Agent API Endpoints","text":"<ul> <li><code>POST /api/agent/run</code> - Execute a task</li> <li><code>POST /api/agent/chat</code> - Conversational interaction</li> <li><code>GET /api/agent/status</code> - Check agent status</li> </ul>"},{"location":"roadmap/#phase-8-client-updates","title":"Phase 8: Client Updates","text":"<p>Goal: Update UI to support new agent capabilities</p>"},{"location":"roadmap/#81-agent-tab","title":"8.1 Agent Tab","text":"<ul> <li>Created AgentTab component</li> <li>Task input with natural language</li> <li>Real-time reasoning trace display</li> <li>Tool usage visualization</li> <li>Cancel/pause controls</li> </ul>"},{"location":"roadmap/#82-dashboard-updates","title":"8.2 Dashboard Updates","text":"<ul> <li>Connected MCP servers status</li> <li>Model provider status (API keys valid, local models loaded)</li> <li>Agent status widget</li> <li>System health overview</li> </ul>"},{"location":"roadmap/#83-settings-tab","title":"8.3 Settings Tab","text":"<ul> <li>Multi-server configuration (per-server settings storage)</li> <li>Model provider API key management</li> <li>Drafting strategy selection</li> <li>Reasoning preferences</li> <li>Memory/storage info</li> </ul>"},{"location":"roadmap/#84-tools-tab-updates","title":"8.4 Tools Tab Updates","text":"<ul> <li>MCP tool browser (all available tools from connected servers)</li> <li>Tool search functionality</li> <li>Tool testing interface (placeholder)</li> <li>Server status display</li> </ul>"},{"location":"roadmap/#85-design-system-updates","title":"8.5 Design System Updates","text":"<ul> <li>Dark cosmic theme (nebula gradients, star accents)</li> <li>Lucide-react icons throughout</li> <li>Glassmorphism effects</li> <li>Glow animations</li> <li>Updated color palette (purple/cyan/pink cosmic theme)</li> </ul>"},{"location":"roadmap/#86-infrastructure","title":"8.6 Infrastructure","text":"<ul> <li>Multi-server storage system (localStorage)</li> <li>ServerContext for app-wide server state</li> <li>Typed API client with React hooks</li> <li>Per-server metadata and preferences</li> </ul>"},{"location":"roadmap/#phase-9-security-infrastructure","title":"Phase 9: Security &amp; Infrastructure","text":"<p>Goal: Secure and stabilize the platform (Foundation)</p>"},{"location":"roadmap/#91-tauri-security","title":"9.1 Tauri Security","text":"<ul> <li>Configured Content Security Policy in <code>tauri.conf.json</code></li> <li>Reviewed and restricted Tauri capabilities (already minimal)</li> <li>Improved window defaults (1200x800, min size)</li> </ul>"},{"location":"roadmap/#92-api-security","title":"9.2 API Security","text":"<ul> <li>Foundation for rate limiting (settings placeholder, not enforced)</li> <li>Foundation for API key authentication (settings placeholder, not enforced)</li> <li>Input validation limits defined (AGENTX_MAX_TEXT_LENGTH, AGENTX_MAX_CHAT_LENGTH)</li> <li>CORS configuration made environment-configurable</li> <li>CORS permissive in DEBUG mode for development</li> </ul> <p>Note: Security is intentionally permissive for private server development. Foundation is in place for future hardening.</p>"},{"location":"roadmap/#phase-10-testing-core","title":"Phase 10: Testing (Core)","text":"<p>Goal: Ensure reliability through automated testing Result: 50 tests, 49 pass, 1 skipped for Docker</p>"},{"location":"roadmap/#101-backend-tests","title":"10.1 Backend Tests","text":"<ul> <li>Fixed and unskipped <code>test_language_detect</code> (POST and GET both work)</li> <li>Translation tests: multiple language pairs, error handling, long text</li> <li>MCP tool tests: servers/tools/resources endpoints, registry operations</li> <li>Reasoning framework tests: base classes, CoT, ToT, ReAct, Reflection</li> <li>Drafting strategy tests: speculative, pipeline, candidate generation</li> <li>Provider tests: registry, model config, Message/CompletionResult</li> </ul>"},{"location":"roadmap/#102-integration-tests","title":"10.2 Integration Tests","text":"<ul> <li>Full translation flow (via API endpoint tests)</li> <li>Docker service dependencies (health check with skipUnless)</li> </ul>"},{"location":"roadmap/#decision-log","title":"Decision Log","text":"Date Decision Rationale 2026-01-21 MCP Client (not server) AgentX consumes tools from external MCP servers 2026-01-21 Multi-strategy drafting Support speculative, pipeline, and candidate generation 2026-01-21 Flexible reasoning Support CoT, ToT, ReAct, Reflection patterns 2026-01-21 Keep agent_memory architecture Well-designed, integrates with reasoning 2026-01-21 Use Todo.md for tracking Simple, version controlled 2026-01-31 Extensible memory over backwards-compat Memory system prioritizes easy extension; no backwards-compatibility shims 2026-01-31 Audit logging in PostgreSQL All memory operations traceable per session with configurable verbosity 2026-01-31 Tenant isolation required Semantic memory must filter by user_id to prevent cross-user data leakage 2026-01-31 Extraction via LLM preferred Entity/fact extraction uses model providers for quality; spaCy as lightweight fallback 2026-01-31 Memory channels over multiple databases Logical channel scoping (property on nodes/rows) vs multiple DBs 2026-01-31 <code>_global</code> as default channel User-wide memory (preferences, general facts) lives in <code>_global</code> 2026-01-31 Channels are traceable scopes, not isolation Retrieval merges active channel + <code>_global</code> 2026-01-31 Cross-channel promotion via consolidation Prominent project facts auto-promote to <code>_global</code> 2026-01-31 Audit logs partitioned by day Configurable retention with daily resolution 2026-01-31 LLM-only extraction, no spaCy Entity/fact extraction uses model providers exclusively 2026-02-13 Dual interface: Chat vs Agent Chat = minimal friction; Agent = full power for prompt engineering 2026-02-13 Agent Profiles as first-class concept Stored configurations that personalize agent behavior 2026-02-13 Conversation branching over linear history Power users need to explore alternatives 2026-02-13 Profile inheritance via <code>extends</code> Child profiles inherit parent settings with overrides 2026-02-13 Global prompt library with tags Templates shared across profiles 2026-02-22 Scheduler deferred, manual trigger priority Manual consolidation trigger for debugging 2026-02-23 LLM providers for consolidation stages Route through existing provider system 2026-02-23 Filter consolidation to user turns only Extract facts from user messages only 2026-02-23 Default memory channel is _default, not _global <code>_global</code> populated via promotion"},{"location":"roadmap/#resolved-questions","title":"Resolved Questions","text":"Question Resolution Which LLM providers to prioritize? OpenAI, Anthropic, Ollama implemented; LM-Studio preferred Should agent memory require authentication? No, one server = one user. Multiple servers can exist on one system. Target platforms for distribution? Linux and Windows for now Reasoning trace storage format? Neo4j graph + PostgreSQL audit log Entity extraction method? LLM-only via model providers; spaCy too constraining Audit log retention policy? 30 days default, configurable with daily resolution Memory retrieval sync or async? Blocking for now, may scale to concurrent for complex tasks Cross-channel promotion thresholds? confidence&gt;=0.85, access_count&gt;=5, conversations&gt;=2"},{"location":"roadmap/#deferred-items","title":"Deferred Items","text":"<p>These items were identified during development but deferred to future phases:</p>"},{"location":"roadmap/#from-phase-3-mcp-client","title":"From Phase 3 (MCP Client)","text":"<ul> <li>UI for managing MCP server connections</li> <li>Test with standard MCP servers (filesystem, github, postgres, brave-search)</li> <li>Document tested/supported servers</li> <li>Custom MCP server templates</li> <li>Tool refresh and search/filter functionality</li> </ul>"},{"location":"roadmap/#from-phase-4-model-providers","title":"From Phase 4 (Model Providers)","text":"<ul> <li>TogetherProvider - Together.ai API</li> <li>LocalTransformersProvider - HuggingFace transformers</li> <li>Cost tracking and budgeting</li> </ul>"},{"location":"roadmap/#from-phase-6-reasoning","title":"From Phase 6 (Reasoning)","text":"<ul> <li>DebateReasoner implementation</li> </ul>"},{"location":"roadmap/#from-phase-9-security","title":"From Phase 9 (Security)","text":"<ul> <li>Secure storage for API keys (Keyring/OS keychain)</li> <li>Encryption for sensitive config</li> <li>Audit logging for sensitive operations</li> </ul>"},{"location":"roadmap/#from-phase-10-testing","title":"From Phase 10 (Testing)","text":"<ul> <li>React component tests</li> <li>E2E tests with Playwright/Cypress</li> </ul>"},{"location":"api/endpoints/","title":"API Endpoints","text":"<p>REST API endpoint reference.</p>"},{"location":"api/endpoints/#base-url","title":"Base URL","text":"<pre><code>http://localhost:12319/api/\n</code></pre>"},{"location":"api/endpoints/#core-endpoints","title":"Core Endpoints","text":""},{"location":"api/endpoints/#index-hello","title":"Index / Hello","text":"<pre><code>GET /api/index\n</code></pre> <p>Returns a simple hello message.</p>"},{"location":"api/endpoints/#health-check","title":"Health Check","text":"<pre><code>GET /api/health\nGET /api/health?include_memory=true\n</code></pre> <p>Returns API and service health status. Add <code>include_memory=true</code> to check database connections.</p> <p>Response: <pre><code>{\n  \"status\": \"healthy\",\n  \"api\": {\"status\": \"healthy\"},\n  \"translation\": {\"status\": \"healthy\", \"models\": {...}},\n  \"memory\": {\n    \"neo4j\": {\"status\": \"healthy\"},\n    \"postgres\": {\"status\": \"healthy\"},\n    \"redis\": {\"status\": \"healthy\"}\n  }\n}\n</code></pre></p>"},{"location":"api/endpoints/#translation-endpoints","title":"Translation Endpoints","text":""},{"location":"api/endpoints/#language-detection","title":"Language Detection","text":"<pre><code>GET  /api/tools/language-detect-20\nPOST /api/tools/language-detect-20\nContent-Type: application/json\n\n{\"text\": \"Bonjour, comment allez-vous?\"}\n</code></pre> <p>Detects language from text (supports ~20 languages for fast detection).</p> <p>Response: <pre><code>{\n  \"original\": \"Bonjour, comment allez-vous?\",\n  \"detected_language\": \"fr\",\n  \"confidence\": 98.5\n}\n</code></pre></p>"},{"location":"api/endpoints/#translation","title":"Translation","text":"<pre><code>POST /api/tools/translate\nContent-Type: application/json\n\n{\n  \"text\": \"Hello, world!\",\n  \"targetLanguage\": \"fra_Latn\"\n}\n</code></pre> <p>Translates text to target language using NLLB-200 language codes (e.g., <code>eng_Latn</code>, <code>fra_Latn</code>, <code>deu_Latn</code>).</p> <p>Response: <pre><code>{\n  \"original\": \"Hello, world!\",\n  \"translatedText\": \"Bonjour, monde!\",\n  \"targetLanguage\": \"fra_Latn\"\n}\n</code></pre></p>"},{"location":"api/endpoints/#mcp-endpoints","title":"MCP Endpoints","text":""},{"location":"api/endpoints/#list-mcp-servers","title":"List MCP Servers","text":"<pre><code>GET /api/mcp/servers\n</code></pre> <p>Returns configured MCP server connections.</p>"},{"location":"api/endpoints/#list-mcp-tools","title":"List MCP Tools","text":"<pre><code>GET /api/mcp/tools\n</code></pre> <p>Returns available tools from connected MCP servers.</p>"},{"location":"api/endpoints/#list-mcp-resources","title":"List MCP Resources","text":"<pre><code>GET /api/mcp/resources\n</code></pre> <p>Returns available resources from connected MCP servers.</p>"},{"location":"api/endpoints/#provider-endpoints","title":"Provider Endpoints","text":""},{"location":"api/endpoints/#list-providers","title":"List Providers","text":"<pre><code>GET /api/providers\n</code></pre> <p>Returns configured model providers (OpenAI, Anthropic, Ollama).</p>"},{"location":"api/endpoints/#list-models","title":"List Models","text":"<pre><code>GET /api/providers/models\nGET /api/providers/models?provider=openai\n</code></pre> <p>Returns available models. Filter by provider with query parameter.</p>"},{"location":"api/endpoints/#provider-health","title":"Provider Health","text":"<pre><code>GET /api/providers/health\n</code></pre> <p>Returns health status of all configured providers.</p>"},{"location":"api/endpoints/#agent-endpoints","title":"Agent Endpoints","text":""},{"location":"api/endpoints/#run-task","title":"Run Task","text":"<pre><code>POST /api/agent/run\nContent-Type: application/json\n\n{\n  \"task\": \"Analyze the sentiment of this text: I love this product!\",\n  \"reasoning_strategy\": \"chain_of_thought\"\n}\n</code></pre> <p>Executes a task using the agent with optional reasoning strategy.</p>"},{"location":"api/endpoints/#chat","title":"Chat","text":"<pre><code>POST /api/agent/chat\nContent-Type: application/json\n\n{\n  \"message\": \"What can you help me with?\",\n  \"session_id\": \"optional-session-uuid\"\n}\n</code></pre> <p>Conversational interaction with session management.</p>"},{"location":"api/endpoints/#agent-status","title":"Agent Status","text":"<pre><code>GET /api/agent/status\n</code></pre> <p>Returns current agent status (idle, running, etc.).</p>"},{"location":"api/endpoints/#authentication","title":"Authentication","text":"<p>Currently no authentication required (development mode).</p> <p>Production Security</p> <p>Enable authentication before deploying to production. See Security Guide.</p>"},{"location":"api/models/","title":"API Models","text":"<p>Data models and schemas used by the API.</p>"},{"location":"api/models/#translation-request","title":"Translation Request","text":"<pre><code>{\n  \"text\": \"string (required)\",\n  \"target_language\": \"string (required, ISO 639-1)\",\n  \"source_language\": \"string (optional, auto-detect)\"\n}\n</code></pre>"},{"location":"api/models/#translation-response","title":"Translation Response","text":"<pre><code>{\n  \"original\": \"string\",\n  \"translated\": \"string\",\n  \"source_language\": \"string\",\n  \"target_language\": \"string\",\n  \"confidence\": \"float (optional)\"\n}\n</code></pre>"},{"location":"api/models/#language-detection","title":"Language Detection","text":"<pre><code>{\n  \"text\": \"string\",\n  \"detected_language\": \"string (ISO 639-1)\",\n  \"confidence\": \"float\"\n}\n</code></pre>"},{"location":"architecture/api/","title":"API Layer","text":"<p>The Django REST API provides AI-powered services for translation and memory management.</p>"},{"location":"architecture/api/#overview","title":"Overview","text":"<ul> <li>Framework: Django 5.2.8</li> <li>Port: 12319</li> <li>Base URL: <code>http://localhost:12319/api/</code></li> </ul>"},{"location":"architecture/api/#key-components","title":"Key Components","text":""},{"location":"architecture/api/#translation-kit-apiagentx_aikittranslationpy","title":"Translation Kit (<code>api/agentx_ai/kit/translation.py</code>)","text":"<p>Handles language detection and translation using HuggingFace models.</p>"},{"location":"architecture/api/#conversation-system-apiagentx_aikitconversationpy","title":"Conversation System (<code>api/agentx_ai/kit/conversation.py</code>)","text":"<p>Manages conversation state and context (planned).</p>"},{"location":"architecture/api/#memory-graph-apiagentx_aikitlibmemory_graphpy","title":"Memory Graph (<code>api/agentx_ai/kit/lib/memory_graph.py</code>)","text":"<p>Graph-based memory system using Neo4j (in development).</p>"},{"location":"architecture/api/#api-endpoints","title":"API Endpoints","text":"<p>See API Reference for complete endpoint documentation.</p>"},{"location":"architecture/api/#next-steps","title":"Next Steps","text":"<ul> <li>API Endpoints - Complete API reference</li> <li>Database Integration - How API connects to databases</li> </ul>"},{"location":"architecture/client/","title":"Client Layer","text":"<p>The Tauri desktop application provides the user interface for AgentX.</p>"},{"location":"architecture/client/#technology-stack","title":"Technology Stack","text":"<ul> <li>Desktop Framework: Tauri v2</li> <li>Frontend: React 19 with TypeScript</li> <li>Build Tool: Vite</li> <li>Styling: CSS Modules (or your choice)</li> </ul>"},{"location":"architecture/client/#project-structure","title":"Project Structure","text":"<pre><code>client/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 App.tsx              # Main app component\n\u2502   \u251c\u2500\u2500 components/          # React components\n\u2502   \u2502   \u251c\u2500\u2500 TabBar.tsx       # Tab navigation\n\u2502   \u2502   \u2514\u2500\u2500 tabs/            # Tab components\n\u2502   \u2514\u2500\u2500 main.tsx             # Entry point\n\u251c\u2500\u2500 src-tauri/               # Rust/Tauri backend\n\u2502   \u251c\u2500\u2500 Cargo.toml\n\u2502   \u251c\u2500\u2500 tauri.conf.json\n\u2502   \u2514\u2500\u2500 src/\n\u2514\u2500\u2500 vite.config.ts\n</code></pre>"},{"location":"architecture/client/#tab-system","title":"Tab System","text":"<p>Four main tabs with persistent state:</p> <ol> <li>Dashboard - Overview and stats</li> <li>Translation - Interactive translation</li> <li>Chat - AI conversations</li> <li>Tools - Utilities and settings</li> </ol> <p>All tabs remain mounted; visibility controlled via CSS.</p>"},{"location":"architecture/client/#development","title":"Development","text":"<ul> <li>Dev server: <code>http://localhost:1420</code> (Vite)</li> <li>HMR enabled for fast iteration</li> <li>Tauri window wraps the Vite dev server</li> </ul> <p>See Development Setup for more information.</p>"},{"location":"architecture/databases/","title":"Database Stack","text":"<p>AgentX uses a hybrid database architecture combining graph, relational, and in-memory databases.</p>"},{"location":"architecture/databases/#overview","title":"Overview","text":"<pre><code>graph TB\n    API[Django API]\n    Neo4j[Neo4j&lt;br/&gt;Graph Database]\n    Postgres[PostgreSQL&lt;br/&gt;+ pgvector]\n    Redis[Redis&lt;br/&gt;Cache]\n\n    API --&gt;|Relationships| Neo4j\n    API --&gt;|Embeddings| Postgres\n    API --&gt;|Cache| Redis\n\n    style Neo4j fill:#4581C3\n    style Postgres fill:#336791\n    style Redis fill:#DC382D</code></pre>"},{"location":"architecture/databases/#database-roles","title":"Database Roles","text":""},{"location":"architecture/databases/#neo4j-graph-database","title":"Neo4j - Graph Database","text":"<p>Purpose: Store and analyze relationships between entities</p> <p>Use Cases:</p> <ul> <li>Entity relationship mapping</li> <li>Knowledge graph construction</li> <li>Semantic connections</li> <li>Graph-based reasoning</li> </ul> <p>Configuration:</p> <ul> <li>Port: 7474 (browser), 7687 (bolt)</li> <li>Data: <code>./data/neo4j/data</code></li> <li>Logs: <code>./data/neo4j/logs</code></li> <li>Plugins: APOC</li> </ul> <p>Example Query:</p> <pre><code>// Create entity with relationships\nCREATE (p:Person {name: 'Alice'})\nCREATE (c:Concept {name: 'Machine Learning'})\nCREATE (p)-[:INTERESTED_IN]-&gt;(c)\n</code></pre>"},{"location":"architecture/databases/#postgresql-pgvector","title":"PostgreSQL + pgvector","text":"<p>Purpose: Store vector embeddings for semantic search</p> <p>Use Cases:</p> <ul> <li>Embedding storage</li> <li>Similarity search</li> <li>Conversation history</li> <li>Structured data</li> </ul> <p>Configuration:</p> <ul> <li>Port: 5432</li> <li>Database: <code>agent_memory</code></li> <li>User: <code>agent</code></li> <li>Data: <code>./data/postgres</code></li> <li>Extensions: pgvector</li> </ul> <p>Example Usage:</p> <pre><code>-- Create embeddings table\nCREATE TABLE embeddings (\n    id SERIAL PRIMARY KEY,\n    content TEXT,\n    embedding vector(768),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Create vector index\nCREATE INDEX ON embeddings\nUSING ivfflat (embedding vector_cosine_ops);\n\n-- Find similar vectors\nSELECT content,\n       embedding &lt;=&gt; '[0.1, 0.2, ...]' AS distance\nFROM embeddings\nORDER BY distance\nLIMIT 10;\n</code></pre>"},{"location":"architecture/databases/#redis-cache-layer","title":"Redis - Cache Layer","text":"<p>Purpose: High-speed caching and temporary data storage</p> <p>Use Cases:</p> <ul> <li>Translation cache</li> <li>Session storage</li> <li>Rate limiting</li> <li>Real-time data</li> </ul> <p>Configuration:</p> <ul> <li>Port: 6379</li> <li>Data: <code>./data/redis</code></li> <li>Max Memory: 512MB</li> <li>Eviction: allkeys-lru</li> </ul> <p>Example Usage:</p> <pre><code>import redis\n\nr = redis.Redis(host='localhost', port=6379)\n\n# Cache translation\nr.setex('translate:en:fr:hello', 3600, 'bonjour')\n\n# Retrieve cached value\ncached = r.get('translate:en:fr:hello')\n</code></pre>"},{"location":"architecture/databases/#data-flow","title":"Data Flow","text":""},{"location":"architecture/databases/#translation-request","title":"Translation Request","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant API\n    participant Redis\n    participant Model\n    participant Neo4j\n\n    Client-&gt;&gt;API: POST /translate\n    API-&gt;&gt;Redis: Check cache\n\n    alt Cache Hit\n        Redis--&gt;&gt;API: Return cached\n    else Cache Miss\n        API-&gt;&gt;Model: Translate\n        Model--&gt;&gt;API: Result\n        API-&gt;&gt;Redis: Store cache\n        API-&gt;&gt;Neo4j: Store relationship\n    end\n\n    API--&gt;&gt;Client: Translation</code></pre>"},{"location":"architecture/databases/#memory-storage","title":"Memory Storage","text":"<pre><code>sequenceDiagram\n    participant API\n    participant Postgres\n    participant Neo4j\n    participant Redis\n\n    API-&gt;&gt;Postgres: Store embedding\n    API-&gt;&gt;Neo4j: Create relationships\n    API-&gt;&gt;Redis: Cache metadata\n\n    Note over Postgres,Redis: Coordinated storage</code></pre>"},{"location":"architecture/databases/#data-directories","title":"Data Directories","text":"<p>All database data is stored in local bind mounts:</p> <pre><code>data/\n\u251c\u2500\u2500 neo4j/\n\u2502   \u251c\u2500\u2500 data/          # Graph database files\n\u2502   \u251c\u2500\u2500 logs/          # Application logs\n\u2502   \u2514\u2500\u2500 plugins/       # APOC and other plugins\n\u251c\u2500\u2500 postgres/          # PostgreSQL data files\n\u2514\u2500\u2500 redis/             # Redis persistence files\n</code></pre>"},{"location":"architecture/databases/#benefits","title":"Benefits","text":"<ul> <li>Easy Backups: Simple directory copy</li> <li>Portability: Move between environments</li> <li>Debugging: Direct file access</li> <li>No Orphaned Volumes: Explicit data location</li> </ul>"},{"location":"architecture/databases/#migration-from-docker-volumes","title":"Migration from Docker Volumes","text":"<p>If you have existing data in Docker volumes:</p> <pre><code># Migrate all databases\ntask db:migrate-volumes\n\n# Or individually\ntask db:migrate-volumes:neo4j\ntask db:migrate-volumes:postgres\ntask db:migrate-volumes:redis\n</code></pre> <p>This copies data from Docker volumes to <code>./data/</code> directories.</p>"},{"location":"architecture/databases/#backup-strategies","title":"Backup Strategies","text":""},{"location":"architecture/databases/#postgresql-backup","title":"PostgreSQL Backup","text":"<p>Automated backup: <pre><code>task db:backup:postgres\n</code></pre></p> <p>Manual backup: <pre><code>docker exec agent-postgres pg_dump -U agent agent_memory &gt; backup.sql\n</code></pre></p> <p>Restore: <pre><code>task db:restore:postgres BACKUP_FILE=backup.sql\n</code></pre></p>"},{"location":"architecture/databases/#neo4j-backup","title":"Neo4j Backup","text":"<p>Using Neo4j dump: <pre><code>docker exec agent-neo4j neo4j-admin database dump neo4j --to-path=/backups\n</code></pre></p>"},{"location":"architecture/databases/#redis-backup","title":"Redis Backup","text":"<p>Redis automatically persists with AOF: <pre><code># Copy the appendonly.aof file\ncp data/redis/appendonlydir/appendonly.aof.1.base.rdb backups/\n</code></pre></p>"},{"location":"architecture/databases/#full-backup","title":"Full Backup","text":"<p>Copy entire data directory: <pre><code>tar -czf agentx-backup-$(date +%Y%m%d).tar.gz data/\n</code></pre></p>"},{"location":"architecture/databases/#database-maintenance","title":"Database Maintenance","text":""},{"location":"architecture/databases/#postgresql","title":"PostgreSQL","text":"<p>Vacuum and analyze: <pre><code>docker exec agent-postgres psql -U agent -d agent_memory -c \"VACUUM ANALYZE;\"\n</code></pre></p>"},{"location":"architecture/databases/#neo4j","title":"Neo4j","text":"<p>Check database size: <pre><code>CALL dbms.queryJmx(\"org.neo4j:instance=kernel#0,name=Store sizes\")\nYIELD attributes\nRETURN attributes.TotalStoreSize.value AS size\n</code></pre></p>"},{"location":"architecture/databases/#redis","title":"Redis","text":"<p>Get memory stats: <pre><code>docker exec agent-redis redis-cli INFO memory\n</code></pre></p>"},{"location":"architecture/databases/#connection-strings","title":"Connection Strings","text":""},{"location":"architecture/databases/#neo4j_1","title":"Neo4j","text":"<pre><code>bolt://localhost:7687\n</code></pre> <p>Python (neo4j driver): <pre><code>from neo4j import GraphDatabase\n\ndriver = GraphDatabase.driver(\n    \"bolt://localhost:7687\",\n    auth=(\"neo4j\", \"your_secure_password\")\n)\n</code></pre></p>"},{"location":"architecture/databases/#postgresql_1","title":"PostgreSQL","text":"<pre><code>postgresql://agent:your_secure_password@localhost:5432/agent_memory\n</code></pre> <p>Python (psycopg2): <pre><code>import psycopg2\n\nconn = psycopg2.connect(\n    host=\"localhost\",\n    port=5432,\n    database=\"agent_memory\",\n    user=\"agent\",\n    password=\"your_secure_password\"\n)\n</code></pre></p>"},{"location":"architecture/databases/#redis_1","title":"Redis","text":"<pre><code>redis://localhost:6379\n</code></pre> <p>Python (redis-py): <pre><code>import redis\n\nr = redis.Redis(\n    host='localhost',\n    port=6379,\n    decode_responses=True\n)\n</code></pre></p>"},{"location":"architecture/databases/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/databases/#neo4j_2","title":"Neo4j","text":"<ul> <li>Default heap: 512MB initial, 2GB max</li> <li>Page cache: 1GB</li> <li>Increase for larger graphs</li> </ul>"},{"location":"architecture/databases/#postgresql_2","title":"PostgreSQL","text":"<ul> <li>Shared buffers: 25% of RAM</li> <li>Work memory: 4MB per connection</li> <li>Maintenance work memory: 64MB</li> </ul>"},{"location":"architecture/databases/#redis_2","title":"Redis","text":"<ul> <li>Max memory: 512MB (configurable)</li> <li>Eviction policy: allkeys-lru</li> <li>Persistence: AOF enabled</li> </ul>"},{"location":"architecture/databases/#security","title":"Security","text":""},{"location":"architecture/databases/#network-isolation","title":"Network Isolation","text":"<p>Databases are exposed on localhost only. For production:</p> <pre><code># docker-compose.yml\nservices:\n  postgres:\n    ports: []  # Remove port mapping\n    networks:\n      - internal  # Use internal network\n</code></pre>"},{"location":"architecture/databases/#authentication","title":"Authentication","text":"<p>Change default passwords in <code>docker-compose.yml</code>:</p> <pre><code>environment:\n  - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}  # Use env var\n</code></pre>"},{"location":"architecture/databases/#encryption","title":"Encryption","text":"<p>For production, enable:</p> <ul> <li>SSL/TLS for PostgreSQL</li> <li>Bolt encryption for Neo4j</li> <li>Redis AUTH password</li> </ul>"},{"location":"architecture/databases/#monitoring","title":"Monitoring","text":""},{"location":"architecture/databases/#health-checks","title":"Health Checks","text":"<p>All services have built-in health checks:</p> <pre><code>docker ps\n</code></pre> <p>Shows health status for each container.</p>"},{"location":"architecture/databases/#database-shells","title":"Database Shells","text":"<p>Access databases directly:</p> <pre><code># PostgreSQL\ntask db:shell:postgres\n\n# Redis\ntask db:shell:redis\n\n# Neo4j Browser\nopen http://localhost:7474\n</code></pre>"},{"location":"architecture/databases/#troubleshooting","title":"Troubleshooting","text":""},{"location":"architecture/databases/#connection-refused","title":"Connection Refused","text":"<p>Check if services are running: <pre><code>docker-compose ps\n</code></pre></p>"},{"location":"architecture/databases/#data-corruption","title":"Data Corruption","text":"<p>Restore from backup: <pre><code>task db:restore:postgres BACKUP_FILE=backups/latest.sql\n</code></pre></p>"},{"location":"architecture/databases/#disk-space","title":"Disk Space","text":"<p>Check data directory sizes: <pre><code>du -sh data/*\n</code></pre></p>"},{"location":"architecture/databases/#performance-issues","title":"Performance Issues","text":"<p>Check resource usage: <pre><code>docker stats\n</code></pre></p>"},{"location":"architecture/databases/#next-steps","title":"Next Steps","text":"<ul> <li>Database Migration Guide - Data migration strategies</li> <li>API Reference - Database interactions via API</li> <li>Development Tasks - Database management commands</li> </ul>"},{"location":"architecture/memory/","title":"Memory System Architecture","text":"<p>Technical architecture documentation for AgentX's cognitive memory system.</p>"},{"location":"architecture/memory/#design-principles","title":"Design Principles","text":"<ul> <li>Extensibility: Easy to add new memory types, stores, or extraction methods without backwards-compatibility constraints</li> <li>Transparency: All memory operations traceable per session/conversation based on logging settings</li> <li>Auditability: Full query trace and operation audit trail in PostgreSQL</li> <li>Channel Scoping: Memory organized into channels \u2014 <code>_global</code> (default, user-wide) and project channels</li> </ul>"},{"location":"architecture/memory/#system-architecture","title":"System Architecture","text":"<p>The memory system is organized into four layers:</p>"},{"location":"architecture/memory/#1-interface-layer","title":"1. Interface Layer","text":"<ul> <li>AgentMemory - Main unified API</li> <li>Provides high-level operations for storing and retrieving memories</li> <li>Manages coordination between different memory types</li> <li>Accepts <code>channel</code> parameter (default <code>\"_global\"</code>) for memory scoping</li> <li>Goal lifecycle management (add, get, complete)</li> </ul>"},{"location":"architecture/memory/#2-memory-subsystems","title":"2. Memory Subsystems","text":""},{"location":"architecture/memory/#episodic-memory-memoryepisodicpy","title":"Episodic Memory (<code>memory/episodic.py</code>)","text":"<p>Stores conversation history with full context:</p> <pre><code>class EpisodicMemory:\n    def store_turn(turn: Turn, channel: str = \"_global\") -&gt; None\n    def store_turn_log(turn: Turn, channel: str = \"_global\") -&gt; None\n    def vector_search(query_embedding, top_k, user_id, channels, time_window_hours) -&gt; List[Dict]\n    def get_conversation(conversation_id: str) -&gt; List[Turn]\n    def get_recent_turns(user_id, channels, hours, limit) -&gt; List[Dict]\n</code></pre> <p>Storage: - Neo4j: Graph structure with Turn nodes and relationships (with channel property) - PostgreSQL: Audit log and time-series backup (with channel column)</p> <p>Indexing: - Vector index on turn embeddings (1536 dimensions) - BRIN index on timestamps for efficient time-range queries - B-tree index on conversation_id and channel</p>"},{"location":"architecture/memory/#semantic-memory-memorysemanticpy","title":"Semantic Memory (<code>memory/semantic.py</code>)","text":"<p>Manages entities, facts, and conceptual knowledge:</p> <pre><code>class SemanticMemory:\n    def upsert_entity(entity: Entity, channel: str = \"_global\") -&gt; Entity\n    def store_fact(fact: Fact, channel: str = \"_global\") -&gt; None\n    def vector_search_facts(query_embedding, top_k, channels, min_confidence) -&gt; List[Dict]\n    def vector_search_entities(query_embedding, top_k, channels) -&gt; List[Dict]\n    def get_entity_graph(entity_ids, depth) -&gt; Dict\n    def create_relationship(source_id, target_id, rel_type, properties) -&gt; None\n    def promote_to_global(fact_id: str, reason: str) -&gt; None\n</code></pre> <p>Storage: - Neo4j: Entity and Fact nodes with typed relationships and channel property - Supports multi-hop graph traversal</p> <p>Key Relationships: - <code>RELATED_TO</code>, <code>PART_OF</code>, <code>LOCATED_IN</code>, <code>WORKS_FOR</code>, <code>KNOWS</code>, <code>CREATED_BY</code>, <code>REFERENCES</code> - <code>DERIVED_FROM</code> (Fact \u2192 Turn) - <code>ABOUT</code> (Fact \u2192 Entity) - <code>PROMOTED_FROM</code> (Fact in _global \u2192 original Fact in project channel)</p>"},{"location":"architecture/memory/#procedural-memory-memoryproceduralpy","title":"Procedural Memory (<code>memory/procedural.py</code>)","text":"<p>Tracks tool usage and successful strategies:</p> <pre><code>class ProceduralMemory:\n    def record_invocation(conversation_id, turn_id, tool_name, tool_input,\n                         tool_output, success, latency_ms, channel) -&gt; None\n    def learn_strategy(description, context_pattern, tool_sequence,\n                       from_conversation_id, success, channel) -&gt; Strategy\n    def find_strategies(task_description, channels, top_k) -&gt; List[Strategy]\n    def reinforce_strategy(strategy_id, success) -&gt; None\n    def get_tool_stats(task_type, channel) -&gt; List[Dict]\n</code></pre> <p>Storage: - Neo4j: Strategy and Tool nodes with performance metrics and channel property - PostgreSQL: Tool invocation audit log with channel column</p> <p>Learning Mechanism: - Tracks success/failure counts for strategies - Calculates success rates for recommendation - Links strategies to task types and tool sequences</p>"},{"location":"architecture/memory/#working-memory-memoryworkingpy","title":"Working Memory (<code>memory/working.py</code>)","text":"<p>Fast, ephemeral storage for current session:</p> <pre><code>class WorkingMemory:\n    # Key pattern: working:{user_id}:{channel}:{conversation_id}:*\n    def add_turn(turn) -&gt; None\n    def get_recent_turns(limit) -&gt; List[Dict]\n    def set(key, value, ttl_seconds) -&gt; None\n    def get(key) -&gt; Optional[Any]\n    def delete(key) -&gt; None\n    def get_context() -&gt; Dict\n    def clear_session() -&gt; None\n    def set_active_goal(goal_id, goal_description) -&gt; None\n    def get_active_goal() -&gt; Optional[Dict]\n    def push_thought(thought) -&gt; None\n    def get_thoughts(limit) -&gt; List[Dict]\n</code></pre> <p>Storage: - Redis lists for turns and thoughts (keys include channel segment) - Redis strings (with TTL) for context values - Automatic expiration after 1 hour (configurable)</p> <p>Key Patterns: - <code>working:{user_id}:{channel}:{conversation_id}:turns</code> - Recent turns - <code>working:{user_id}:{channel}:{conversation_id}:context</code> - Context window - <code>consolidation:job:{job_id}</code> - Consolidation job tracking - <code>consolidation:lock:{conversation_id}</code> - Consolidation locks - <code>session:{session_id}:*</code> - Session-scoped ephemeral data</p>"},{"location":"architecture/memory/#3-retrieval-layer","title":"3. Retrieval Layer","text":"<p>Multi-strategy retrieval engine combines:</p> <pre><code>class MemoryRetriever:\n    def retrieve(query, user_id, channels, top_k, include_episodic,\n                include_semantic, include_procedural,\n                time_window_hours) -&gt; MemoryBundle\n</code></pre> <p>Channel Behavior: - Retrieval always queries both the active channel AND <code>_global</code> - Results are merged and deduplicated - Cross-channel matches are logged in the audit trail</p> <p>Strategies: 1. Vector Similarity: Embed query and search each memory type 2. Graph Traversal: Expand from matched entities to related nodes 3. Temporal Filtering: Boost recent memories, filter by time window 4. Reranking: Ensure diversity, limit items per conversation</p> <p>Retrieval Weights: - Episodic: 0.3 - Semantic (Facts): 0.25 - Semantic (Entities): 0.2 - Procedural: 0.15 - Recency: 0.1</p>"},{"location":"architecture/memory/#4-background-processing","title":"4. Background Processing","text":"<p>Consolidation worker runs scheduled jobs:</p> <pre><code>class ConsolidationWorker:\n    jobs = {\n        \"consolidate\": {\"func\": consolidate_episodic_to_semantic, \"interval_minutes\": 15},\n        \"patterns\": {\"func\": detect_patterns, \"interval_minutes\": 60},\n        \"decay\": {\"func\": apply_memory_decay, \"interval_minutes\": 1440},\n        \"cleanup\": {\"func\": cleanup_old_memories, \"interval_minutes\": 1440},\n        \"promote\": {\"func\": promote_prominent_facts, \"interval_minutes\": 60},\n        \"audit_partitions\": {\"func\": manage_audit_partitions, \"interval_minutes\": 1440}\n    }\n</code></pre> <p>Jobs: - consolidate_episodic_to_semantic: Extracts entities and facts from conversations - detect_patterns: Learns strategies from successful conversations - apply_memory_decay: Reduces salience/confidence over time - cleanup_old_memories: Archives old conversations, deletes low-salience entities - promote_prominent_facts: Promotes high-confidence project facts to <code>_global</code> - manage_audit_partitions: Creates future partitions, drops old ones per retention policy</p>"},{"location":"architecture/memory/#5-audit-layer","title":"5. Audit Layer","text":"<p>All memory operations are logged to PostgreSQL for traceability:</p> <pre><code>class MemoryAuditLogger:\n    def log_write(operation, memory_type, channel, affected_ids, metadata) -&gt; None\n    def log_read(query, channels_searched, result_count, latency_ms) -&gt; None\n    def log_promotion(fact_id, source_channel, reason, thresholds) -&gt; None\n</code></pre> <p>Log Levels: - <code>off</code>: No audit logging - <code>writes</code>: Only log mutations (default) - <code>reads</code>: Log reads and writes - <code>verbose</code>: Full query traces with payloads</p> <p>Promotion Thresholds (configurable): - <code>confidence &gt;= 0.85</code> - <code>access_count &gt;= 5</code> - <code>conversations &gt;= 2</code></p> <p>Active thresholds are snapshotted in each audit log entry for reproducibility.</p>"},{"location":"architecture/memory/#memory-channels","title":"Memory Channels","text":"<p>Channels organize memory into traceable scopes:</p> Channel Description <code>_global</code> Default channel, user-wide memory (preferences, general facts) <code>&lt;project&gt;</code> Project-specific memory containers (e.g., <code>my-rust-project</code>) <p>Key behaviors: - Channels are traceable scopes, not isolation boundaries - Retrieval queries both active channel + <code>_global</code>, merging results - Cross-channel intersections are logged in audit trail - Prominent project facts auto-promote to <code>_global</code> based on thresholds</p>"},{"location":"architecture/memory/#database-schemas","title":"Database Schemas","text":""},{"location":"architecture/memory/#neo4j-graph-schema","title":"Neo4j Graph Schema","text":""},{"location":"architecture/memory/#node-types","title":"Node Types","text":"<p>Conversation <pre><code>(:Conversation {\n    id: string (unique),\n    user_id: string,\n    channel: string,  -- '_global' or project name\n    started_at: datetime,\n    title: string,\n    consolidated: datetime,\n    patterns_extracted: boolean,\n    archived: boolean\n})\n</code></pre></p> <p>Turn <pre><code>(:Turn {\n    id: string (unique),\n    user_id: string,\n    channel: string,\n    index: integer,\n    timestamp: datetime,\n    role: string,\n    content: text,\n    embedding: vector(1536),\n    token_count: integer,\n    archived: boolean\n})\n</code></pre></p> <p>Entity <pre><code>(:Entity {\n    id: string (unique),\n    user_id: string,\n    channel: string,\n    name: string,\n    type: string,\n    aliases: list&lt;string&gt;,\n    description: text,\n    embedding: vector(1536),\n    salience: float,\n    first_seen: datetime,\n    last_accessed: datetime,\n    access_count: integer,\n    properties: map\n})\n</code></pre></p> <p>Fact <pre><code>(:Fact {\n    id: string (unique),\n    user_id: string,\n    channel: string,\n    claim: text,\n    confidence: float,\n    source: string,\n    source_turn_id: string,\n    embedding: vector(1536),\n    created_at: datetime\n})\n</code></pre></p> <p>Goal <pre><code>(:Goal {\n    id: string (unique),\n    user_id: string,\n    channel: string,\n    description: text,\n    status: string,          -- 'active', 'completed', 'abandoned', 'blocked'\n    priority: integer,\n    created_at: datetime,\n    completed_at: datetime,  -- Set by complete_goal()\n    result: text,            -- Completion result/summary\n    deadline: datetime,\n    embedding: vector(1536)\n})\n</code></pre></p> <p>Goal Lifecycle: - <code>add_goal(goal)</code> \u2192 Creates Goal node, links to User - <code>get_goal(goal_id)</code> \u2192 Retrieves Goal by ID with parent info - <code>complete_goal(goal_id, status, result)</code> \u2192 Updates status, sets completed_at and result - TaskPlanner automatically creates goals on <code>plan()</code> when memory is provided - Agent completes goals on task success/failure in <code>run()</code></p> <p>Strategy <pre><code>(:Strategy {\n    id: string (unique),\n    user_id: string,\n    channel: string,\n    description: text,\n    context_pattern: string,\n    tool_sequence: list&lt;string&gt;,\n    embedding: vector(1536),\n    success_count: integer,\n    failure_count: integer,\n    created_at: datetime,\n    last_used: datetime\n})\n</code></pre></p> <p>Tool <pre><code>(:Tool {\n    name: string (unique),\n    usage_count: integer,\n    success_count: integer,\n    avg_latency_ms: float\n})\n</code></pre></p>"},{"location":"architecture/memory/#relationship-types","title":"Relationship Types","text":"<ul> <li><code>(:Conversation)-[:HAS_TURN]-&gt;(:Turn)</code></li> <li><code>(:Turn)-[:FOLLOWED_BY]-&gt;(:Turn)</code></li> <li><code>(:Conversation)-[:MENTIONS]-&gt;(:Entity)</code></li> <li><code>(:Fact)-[:DERIVED_FROM]-&gt;(:Turn)</code></li> <li><code>(:Fact)-[:ABOUT]-&gt;(:Entity)</code></li> <li><code>(:Fact)-[:PROMOTED_FROM]-&gt;(:Fact)</code> \u2014 Links global fact to original project fact</li> <li><code>(:User)-[:HAS_GOAL]-&gt;(:Goal)</code></li> <li><code>(:Goal)-[:SUBGOAL_OF]-&gt;(:Goal)</code></li> <li><code>(:Goal)-[:BLOCKED_BY]-&gt;(:Goal)</code></li> <li><code>(:Strategy)-[:USES_TOOL]-&gt;(:Tool)</code></li> <li><code>(:Strategy)-[:SUCCEEDED_IN]-&gt;(:Conversation)</code></li> <li><code>(:Conversation)-[:USED_TOOL]-&gt;(:ToolInvocation)-[:INVOKED]-&gt;(:Tool)</code></li> </ul>"},{"location":"architecture/memory/#channel-indexes","title":"Channel Indexes","text":"<pre><code>// Channel indexes for scoped queries\nCREATE INDEX turn_channel IF NOT EXISTS FOR (t:Turn) ON (t.channel);\nCREATE INDEX entity_channel IF NOT EXISTS FOR (e:Entity) ON (e.channel);\nCREATE INDEX fact_channel IF NOT EXISTS FOR (f:Fact) ON (f.channel);\nCREATE INDEX strategy_channel IF NOT EXISTS FOR (s:Strategy) ON (s.channel);\nCREATE INDEX conversation_channel IF NOT EXISTS FOR (c:Conversation) ON (c.channel);\nCREATE INDEX goal_channel IF NOT EXISTS FOR (g:Goal) ON (g.channel);\n\n// Composite indexes for common query patterns\nCREATE INDEX conversation_user_channel IF NOT EXISTS FOR (c:Conversation) ON (c.user_id, c.channel);\nCREATE INDEX entity_user_channel IF NOT EXISTS FOR (e:Entity) ON (e.user_id, e.channel);\nCREATE INDEX fact_user_channel IF NOT EXISTS FOR (f:Fact) ON (f.user_id, f.channel);\n</code></pre>"},{"location":"architecture/memory/#vector-indexes","title":"Vector Indexes","text":"<pre><code>// Turn embeddings (episodic memory)\nCREATE VECTOR INDEX turn_embeddings IF NOT EXISTS\nFOR (t:Turn) ON (t.embedding)\nOPTIONS {indexConfig: {`vector.dimensions`: 1536, `vector.similarity_function`: 'cosine'}};\n\n// Entity embeddings (semantic memory)\nCREATE VECTOR INDEX entity_embeddings IF NOT EXISTS\nFOR (e:Entity) ON (e.embedding)\nOPTIONS {indexConfig: {`vector.dimensions`: 1536, `vector.similarity_function`: 'cosine'}};\n\n// Fact embeddings\nCREATE VECTOR INDEX fact_embeddings IF NOT EXISTS\nFOR (f:Fact) ON (f.embedding)\nOPTIONS {indexConfig: {`vector.dimensions`: 1536, `vector.similarity_function`: 'cosine'}};\n\n// Strategy embeddings (procedural memory)\nCREATE VECTOR INDEX strategy_embeddings IF NOT EXISTS\nFOR (s:Strategy) ON (s.embedding)\nOPTIONS {indexConfig: {`vector.dimensions`: 1536, `vector.similarity_function`: 'cosine'}};\n</code></pre>"},{"location":"architecture/memory/#postgresql-schema","title":"PostgreSQL Schema","text":""},{"location":"architecture/memory/#conversation_logs","title":"conversation_logs","text":"<pre><code>CREATE TABLE conversation_logs (\n    id BIGSERIAL PRIMARY KEY,\n    conversation_id UUID NOT NULL,\n    turn_index INTEGER NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    role VARCHAR(20) NOT NULL,\n    content TEXT NOT NULL,\n    content_hash VARCHAR(64),\n    token_count INTEGER,\n    model VARCHAR(100),\n    channel VARCHAR(100) NOT NULL DEFAULT '_global',\n    metadata JSONB DEFAULT '{}',\n    embedding vector(1536),\n    UNIQUE(conversation_id, turn_index)\n);\n\nCREATE INDEX idx_logs_timestamp ON conversation_logs USING BRIN (timestamp);\nCREATE INDEX idx_logs_conversation ON conversation_logs (conversation_id);\nCREATE INDEX idx_logs_channel ON conversation_logs (channel);\nCREATE INDEX idx_logs_embedding ON conversation_logs USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n</code></pre>"},{"location":"architecture/memory/#memory_timeline","title":"memory_timeline","text":"<pre><code>CREATE TABLE memory_timeline (\n    id BIGSERIAL PRIMARY KEY,\n    memory_type VARCHAR(50) NOT NULL,\n    neo4j_node_id VARCHAR(100),\n    event_time TIMESTAMPTZ NOT NULL,\n    summary TEXT,\n    embedding vector(1536),\n    importance_score FLOAT DEFAULT 0.5,\n    access_count INTEGER DEFAULT 0,\n    last_accessed TIMESTAMPTZ,\n    channel VARCHAR(100) NOT NULL DEFAULT '_global',\n    archived BOOLEAN DEFAULT FALSE,\n    metadata JSONB DEFAULT '{}'\n);\n\nCREATE INDEX idx_timeline_time ON memory_timeline USING BRIN (event_time);\nCREATE INDEX idx_timeline_type ON memory_timeline (memory_type);\nCREATE INDEX idx_timeline_importance ON memory_timeline (importance_score DESC);\nCREATE INDEX idx_timeline_channel ON memory_timeline (channel);\nCREATE INDEX idx_timeline_embedding ON memory_timeline USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n</code></pre>"},{"location":"architecture/memory/#tool_invocations","title":"tool_invocations","text":"<pre><code>CREATE TABLE tool_invocations (\n    id BIGSERIAL PRIMARY KEY,\n    conversation_id UUID NOT NULL,\n    turn_index INTEGER NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    tool_name VARCHAR(100) NOT NULL,\n    tool_input JSONB NOT NULL,\n    tool_output JSONB,\n    success BOOLEAN,\n    latency_ms INTEGER,\n    error_message TEXT,\n    channel VARCHAR(100) NOT NULL DEFAULT '_global'\n);\n\nCREATE INDEX idx_tools_conversation ON tool_invocations (conversation_id);\nCREATE INDEX idx_tools_name ON tool_invocations (tool_name);\nCREATE INDEX idx_tools_timestamp ON tool_invocations USING BRIN (timestamp);\nCREATE INDEX idx_tools_channel ON tool_invocations (channel);\n</code></pre>"},{"location":"architecture/memory/#user_profiles","title":"user_profiles","text":"<pre><code>CREATE TABLE user_profiles (\n    user_id VARCHAR(100) PRIMARY KEY,\n    created_at TIMESTAMPTZ DEFAULT NOW(),\n    updated_at TIMESTAMPTZ DEFAULT NOW(),\n    preferences JSONB DEFAULT '{}',\n    expertise_areas JSONB DEFAULT '[]',\n    communication_style JSONB DEFAULT '{}',\n    metadata JSONB DEFAULT '{}'\n);\n</code></pre>"},{"location":"architecture/memory/#memory_audit_log-partitioned","title":"memory_audit_log (Partitioned)","text":"<pre><code>CREATE TABLE memory_audit_log (\n    id BIGSERIAL,\n    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    operation VARCHAR(50) NOT NULL,      -- 'store', 'retrieve', 'update', 'delete', 'promote'\n    memory_type VARCHAR(50) NOT NULL,    -- 'episodic', 'semantic', 'procedural', 'working'\n    user_id VARCHAR(100),\n    session_id VARCHAR(100),\n    conversation_id UUID,\n    source_channel VARCHAR(100),         -- Channel where operation originated\n    target_channels TEXT[],              -- Channels queried/affected\n    query_text TEXT,                     -- For retrieval: the query used\n    result_count INTEGER,\n    latency_ms INTEGER,\n    success BOOLEAN DEFAULT TRUE,\n    error_message TEXT,\n    -- Promotion tracking\n    promoted_from_channel VARCHAR(100),\n    promotion_confidence FLOAT,\n    promotion_access_count INTEGER,\n    promotion_conversation_count INTEGER,\n    -- Configuration snapshot\n    config_snapshot JSONB DEFAULT '{}',\n    metadata JSONB DEFAULT '{}',\n    PRIMARY KEY (id, timestamp)\n) PARTITION BY RANGE (timestamp);\n\n-- Daily partitions created automatically\n-- Retention managed by drop_old_audit_partitions(retention_days)\n\nCREATE INDEX idx_audit_user ON memory_audit_log (user_id, timestamp);\nCREATE INDEX idx_audit_session ON memory_audit_log (session_id, timestamp);\nCREATE INDEX idx_audit_operation ON memory_audit_log (operation, timestamp);\nCREATE INDEX idx_audit_memory_type ON memory_audit_log (memory_type, timestamp);\nCREATE INDEX idx_audit_source_channel ON memory_audit_log (source_channel, timestamp);\n</code></pre>"},{"location":"architecture/memory/#schema_version","title":"schema_version","text":"<pre><code>CREATE TABLE schema_version (\n    version INTEGER PRIMARY KEY,\n    description TEXT NOT NULL,\n    applied_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n</code></pre>"},{"location":"architecture/memory/#data-flow","title":"Data Flow","text":""},{"location":"architecture/memory/#1-storing-a-conversation-turn","title":"1. Storing a Conversation Turn","text":"<pre><code>User Input \u2192 AgentMemory.store_turn()\n    \u251c\u2500\u2192 Generate embedding (if missing)\n    \u251c\u2500\u2192 EpisodicMemory.store_turn() \u2192 Neo4j (graph structure)\n    \u251c\u2500\u2192 EpisodicMemory.store_turn_log() \u2192 PostgreSQL (audit log)\n    \u2514\u2500\u2192 WorkingMemory.add_turn() \u2192 Redis (session cache)\n</code></pre>"},{"location":"architecture/memory/#2-memory-retrieval","title":"2. Memory Retrieval","text":"<pre><code>Query \u2192 AgentMemory.remember()\n    \u2514\u2500\u2192 MemoryRetriever.retrieve()\n        \u251c\u2500\u2192 Generate query embedding\n        \u251c\u2500\u2192 EpisodicMemory.vector_search() \u2192 relevant turns\n        \u251c\u2500\u2192 SemanticMemory.vector_search_facts() \u2192 relevant facts\n        \u251c\u2500\u2192 SemanticMemory.vector_search_entities() \u2192 relevant entities\n        \u2502   \u2514\u2500\u2192 SemanticMemory.get_entity_graph() \u2192 expand to related\n        \u251c\u2500\u2192 ProceduralMemory.find_strategies() \u2192 applicable strategies\n        \u251c\u2500\u2192 Get active goals and user context\n        \u2514\u2500\u2192 Rerank and return MemoryBundle\n</code></pre>"},{"location":"architecture/memory/#3-background-consolidation","title":"3. Background Consolidation","text":"<pre><code>ConsolidationWorker (every 15 min)\n    \u2514\u2500\u2192 consolidate_episodic_to_semantic()\n        \u251c\u2500\u2192 Query unconsolidated conversations (with user_id, channel)\n        \u251c\u2500\u2192 Create AgentMemory instance per user/channel\n        \u251c\u2500\u2192 Extract entities \u2192 AgentMemory.upsert_entity() (with embeddings)\n        \u251c\u2500\u2192 Extract facts \u2192 AgentMemory.learn_fact() (with embeddings)\n        \u2514\u2500\u2192 Mark conversation as consolidated\n</code></pre> <p>The consolidation job uses the AgentMemory interface rather than direct Neo4j writes, ensuring consistent embedding generation and data model compliance.</p>"},{"location":"architecture/memory/#memory-decay","title":"Memory Decay","text":"<p>Implements forgetting curve based on:</p> <ul> <li>Time since last access: Exponential decay</li> <li>Access frequency: Logarithmic boost</li> <li>Recency: Inverse exponential boost</li> </ul> <pre><code># Exponential decay formula\ndecayed_value = initial_value * (decay_rate ** days_since_access)\n\n# Default decay rate: 0.95 (5% daily decay)\n# Half-life \u2248 14 days\n</code></pre> <p>Decay Targets: - Entity salience: Daily decay for entities not accessed in 24h - Fact confidence: Weekly decay for inferred facts (slower decay)</p>"},{"location":"architecture/memory/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/memory/#vector-search-optimization","title":"Vector Search Optimization","text":"<ul> <li>Use ANN (Approximate Nearest Neighbors) with IVFFlat index</li> <li>Over-fetch (2x top_k) to account for filtering</li> <li>Cache embeddings for frequently accessed items</li> </ul>"},{"location":"architecture/memory/#graph-traversal-limits","title":"Graph Traversal Limits","text":"<ul> <li>Limit depth to 2-3 hops to prevent slow queries</li> <li>Use <code>LIMIT</code> clauses in all Cypher queries</li> <li>Consider materialized paths for deep hierarchies</li> </ul>"},{"location":"architecture/memory/#redis-memory-management","title":"Redis Memory Management","text":"<ul> <li>Use TTLs on all working memory keys</li> <li>Configure LRU eviction policy: <code>maxmemory-policy allkeys-lru</code></li> <li>Monitor memory usage with <code>INFO memory</code></li> </ul>"},{"location":"architecture/memory/#postgresql-time-series-optimization","title":"PostgreSQL Time-Series Optimization","text":"<ul> <li>BRIN indexes for timestamp columns (highly efficient for time-series)</li> <li>Partition large tables by time range</li> <li>Use <code>EXPLAIN ANALYZE</code> to optimize slow queries</li> </ul>"},{"location":"architecture/memory/#scaling-considerations","title":"Scaling Considerations","text":""},{"location":"architecture/memory/#horizontal-scaling","title":"Horizontal Scaling","text":"<ul> <li>Neo4j: Use Neo4j Enterprise with clustering (read replicas)</li> <li>PostgreSQL: Use read replicas for analytics queries</li> <li>Redis: Use Redis Cluster for distributed caching</li> </ul>"},{"location":"architecture/memory/#vertical-scaling","title":"Vertical Scaling","text":"<ul> <li>Neo4j: Allocate 2-4GB heap, 1GB+ page cache</li> <li>PostgreSQL: Tune <code>shared_buffers</code>, <code>work_mem</code>, <code>effective_cache_size</code></li> <li>Redis: Allocate 512MB-2GB depending on working memory size</li> </ul>"},{"location":"architecture/memory/#data-retention","title":"Data Retention","text":"<ul> <li>Archive conversations older than 90 days (configurable)</li> <li>Delete entities with salience &lt; 0.1 not accessed in 30 days</li> <li>Keep audit logs in PostgreSQL for compliance (longer retention)</li> </ul>"},{"location":"architecture/memory/#security-considerations","title":"Security Considerations","text":"<ul> <li>Store embeddings in vector format (not reversible to text)</li> <li>Encrypt sensitive data in PostgreSQL</li> <li>Use connection pooling to prevent exhaustion</li> <li>Implement rate limiting on retrieval operations</li> <li>Sanitize user input to prevent Cypher injection</li> </ul>"},{"location":"architecture/memory/#monitoring","title":"Monitoring","text":"<p>Key metrics to track:</p> <ul> <li>Memory size: Total nodes/relationships in Neo4j</li> <li>Query performance: P50, P95, P99 latencies for retrieval</li> <li>Consolidation lag: Time between conversation end and consolidation</li> <li>Cache hit rate: Redis cache effectiveness</li> <li>Decay rate: Number of entities/facts decayed per day</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>AgentX follows a two-tier architecture with clear separation between API and client layers.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<pre><code>graph TB\n    subgraph \"Client Layer\"\n        Tauri[Tauri v2 Desktop App]\n        React[React 19 + TypeScript]\n        Vite[Vite Build System]\n    end\n\n    subgraph \"API Layer\"\n        Django[Django 5.2.8 REST API]\n        ML[HuggingFace Transformers]\n    end\n\n    subgraph \"Data Layer\"\n        Neo4j[Neo4j Graph DB]\n        Postgres[PostgreSQL + pgvector]\n        Redis[Redis Cache]\n    end\n\n    React --&gt; Tauri\n    Tauri --&gt;|HTTP| Django\n    Django --&gt; ML\n    Django --&gt; Neo4j\n    Django --&gt; Postgres\n    Django --&gt; Redis</code></pre>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":"<ul> <li>Client Layer: Desktop application built with Tauri and React</li> <li>API Layer: Django REST API providing AI services</li> <li>Data Layer: Multi-database stack for different use cases</li> </ul> <p>See API Layer, Client Layer, and Database Stack for details.</p>"},{"location":"deployment/docker/","title":"Docker Deployment","text":"<p>Deploying AgentX with Docker.</p>"},{"location":"deployment/docker/#quick-start","title":"Quick Start","text":"<pre><code># Start all database services\ntask runners\n\n# Or use docker-compose directly\ndocker-compose up -d\n</code></pre>"},{"location":"deployment/docker/#services","title":"Services","text":"<p>The project includes three database services:</p> Service Port Purpose Neo4j 7474 (HTTP), 7687 (Bolt) Graph database for knowledge graphs PostgreSQL 5432 Relational storage with pgvector Redis 6379 Caching and working memory"},{"location":"deployment/docker/#neo4j","title":"Neo4j","text":"<p>Graph database for entity relationships and knowledge graphs.</p> <pre><code># Access Neo4j Browser\nopen http://localhost:7474\n\n# Default credentials (from .env)\n# Username: neo4j\n# Password: changeme\n</code></pre>"},{"location":"deployment/docker/#postgresql-with-pgvector","title":"PostgreSQL with pgvector","text":"<p>Relational database with vector similarity search.</p> <pre><code># Connect via psql\ndocker exec -it agentx-postgres psql -U agent -d agentx_memory\n\n# Default credentials (from .env)\n# Username: agent\n# Password: changeme\n# Database: agentx_memory\n</code></pre>"},{"location":"deployment/docker/#redis","title":"Redis","text":"<p>In-memory cache for working memory and session data.</p> <pre><code># Connect via redis-cli\ndocker exec -it agentx-redis redis-cli\n</code></pre>"},{"location":"deployment/docker/#docker-compose-commands","title":"Docker Compose Commands","text":"<pre><code># Start services\ndocker-compose up -d\n\n# Stop services\ndocker-compose down\n\n# View logs\ndocker-compose logs -f\n\n# View specific service logs\ndocker-compose logs -f neo4j\n\n# Restart a service\ndocker-compose restart postgres\n\n# Remove volumes (WARNING: deletes data)\ndocker-compose down -v\n</code></pre>"},{"location":"deployment/docker/#data-persistence","title":"Data Persistence","text":"<p>Data is stored in local bind mounts under <code>./data/</code>:</p> <pre><code>data/\n\u251c\u2500\u2500 neo4j/\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 logs/\n\u251c\u2500\u2500 postgres/\n\u2502   \u2514\u2500\u2500 data/\n\u2514\u2500\u2500 redis/\n    \u2514\u2500\u2500 data/\n</code></pre>"},{"location":"deployment/docker/#initializing-data-directories","title":"Initializing Data Directories","text":"<pre><code>task db:init\n</code></pre>"},{"location":"deployment/docker/#migrating-from-docker-volumes","title":"Migrating from Docker Volumes","text":"<p>If you previously used Docker volumes:</p> <pre><code>task db:migrate-volumes\n</code></pre>"},{"location":"deployment/docker/#environment-configuration","title":"Environment Configuration","text":"<p>Configure services via <code>.env</code> file (copy from <code>.env.example</code>):</p> <pre><code># Database passwords\nNEO4J_PASSWORD=your_secure_password\nPOSTGRES_PASSWORD=your_secure_password\n\n# Neo4j settings\nNEO4J_AUTH=neo4j/${NEO4J_PASSWORD}\nNEO4J_PLUGINS=[\"apoc\"]\n\n# PostgreSQL settings\nPOSTGRES_USER=agent\nPOSTGRES_DB=agentx_memory\n</code></pre>"},{"location":"deployment/docker/#production-considerations","title":"Production Considerations","text":""},{"location":"deployment/docker/#security-checklist","title":"Security Checklist","text":"<ul> <li> Change all default passwords in <code>.env</code></li> <li> Use strong, unique passwords for each service</li> <li> Restrict network access (don't expose ports publicly)</li> <li> Enable SSL/TLS for database connections</li> <li> Set up firewall rules</li> </ul>"},{"location":"deployment/docker/#backups","title":"Backups","text":"<pre><code># Backup Neo4j\ndocker exec agentx-neo4j neo4j-admin database dump neo4j --to-path=/backups\n\n# Backup PostgreSQL\ndocker exec agentx-postgres pg_dump -U agent agentx_memory &gt; backup.sql\n\n# Backup Redis\ndocker exec agentx-redis redis-cli BGSAVE\n</code></pre>"},{"location":"deployment/docker/#monitoring","title":"Monitoring","text":"<p>Consider adding:</p> <ul> <li>Prometheus metrics exporters</li> <li>Grafana dashboards</li> <li>Log aggregation (ELK stack, Loki)</li> <li>Health check endpoints</li> </ul>"},{"location":"deployment/docker/#resource-limits","title":"Resource Limits","text":"<p>Add resource constraints in <code>docker-compose.yml</code>:</p> <pre><code>services:\n  neo4j:\n    deploy:\n      resources:\n        limits:\n          memory: 2G\n          cpus: '1.0'\n</code></pre>"},{"location":"deployment/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/docker/#port-conflicts","title":"Port Conflicts","text":"<p>If ports are already in use:</p> <pre><code># Check what's using a port\nlsof -i :7474\n\n# Modify ports in docker-compose.yml\nports:\n  - \"17474:7474\"  # Use different host port\n</code></pre>"},{"location":"deployment/docker/#connection-issues","title":"Connection Issues","text":"<pre><code># Verify containers are running\ndocker-compose ps\n\n# Check container logs\ndocker-compose logs neo4j\n\n# Test connectivity\nnc -zv localhost 7687\n</code></pre>"},{"location":"deployment/docker/#data-directory-permissions","title":"Data Directory Permissions","text":"<pre><code># Fix permissions if needed\nsudo chown -R $USER:$USER ./data\n</code></pre>"},{"location":"deployment/docker/#see-also","title":"See Also","text":"<ul> <li>docker-compose.yml</li> <li>Database Migration Guide</li> <li>Memory Setup</li> </ul>"},{"location":"deployment/migration/","title":"Database Migration","text":"<p>Guide for migrating database data.</p>"},{"location":"deployment/migration/#from-docker-volumes-to-bind-mounts","title":"From Docker Volumes to Bind Mounts","text":"<pre><code># Migrate all databases\ntask db:migrate-volumes\n\n# Or individually\ntask db:migrate-volumes:neo4j\ntask db:migrate-volumes:postgres\ntask db:migrate-volumes:redis\n</code></pre>"},{"location":"deployment/migration/#backup-and-restore","title":"Backup and Restore","text":""},{"location":"deployment/migration/#postgresql","title":"PostgreSQL","text":"<p>Backup: <pre><code>task db:backup:postgres\n</code></pre></p> <p>Restore: <pre><code>task db:restore:postgres BACKUP_FILE=backups/postgres_20231127.sql\n</code></pre></p>"},{"location":"deployment/migration/#data-directories","title":"Data Directories","text":"<p>All data stored in <code>./data/</code>:</p> <pre><code>data/\n\u251c\u2500\u2500 neo4j/\n\u251c\u2500\u2500 postgres/\n\u2514\u2500\u2500 redis/\n</code></pre> <p>See Database Stack for details.</p>"},{"location":"development/contributing/","title":"Contributing","text":"<p>Thank you for contributing to AgentX!</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository</li> <li>Clone your fork</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Run tests</li> <li>Submit a pull request</li> </ol>"},{"location":"development/contributing/#development-guidelines","title":"Development Guidelines","text":"<ul> <li>Follow existing code style</li> <li>Write tests for new features</li> <li>Update documentation</li> <li>Keep commits atomic and meaningful</li> </ul>"},{"location":"development/contributing/#code-review-process","title":"Code Review Process","text":"<p>All changes require code review before merging.</p>"},{"location":"development/contributing/#questions","title":"Questions?","text":"<p>Open an issue or discussion on GitHub.</p>"},{"location":"development/memory-setup/","title":"Memory System Setup Guide","text":"<p>Complete setup instructions for the AgentX memory system.</p>"},{"location":"development/memory-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose (for containerized databases)</li> <li>Python 3.11+ with uv or pip</li> <li>OpenAI API key (for embeddings) or local model setup</li> </ul>"},{"location":"development/memory-setup/#quick-start","title":"Quick Start","text":""},{"location":"development/memory-setup/#1-start-database-services","title":"1. Start Database Services","text":"<p>Use the provided Docker Compose configuration:</p> <pre><code># Start all database services\ndocker-compose up -d\n\n# Verify services are running\ndocker-compose ps\n\n# Check logs\ndocker-compose logs -f neo4j postgres redis\n</code></pre>"},{"location":"development/memory-setup/#2-initialize-neo4j-schema","title":"2. Initialize Neo4j Schema","text":"<p>Run the schema initialization script:</p> <pre><code># Connect to Neo4j browser\nopen http://localhost:7474\n\n# Or use cypher-shell\ndocker exec -it agent-neo4j cypher-shell -u neo4j -p your_secure_password\n</code></pre> <p>Execute the following Cypher commands:</p> <pre><code>// ============================================\n// CONSTRAINTS AND INDEXES\n// ============================================\n\n// Uniqueness constraints\nCREATE CONSTRAINT conversation_id IF NOT EXISTS\nFOR (c:Conversation) REQUIRE c.id IS UNIQUE;\n\nCREATE CONSTRAINT entity_id IF NOT EXISTS\nFOR (e:Entity) REQUIRE e.id IS UNIQUE;\n\nCREATE CONSTRAINT fact_id IF NOT EXISTS\nFOR (f:Fact) REQUIRE f.id IS UNIQUE;\n\nCREATE CONSTRAINT goal_id IF NOT EXISTS\nFOR (g:Goal) REQUIRE g.id IS UNIQUE;\n\nCREATE CONSTRAINT user_id IF NOT EXISTS\nFOR (u:User) REQUIRE u.id IS UNIQUE;\n\n// Property indexes for fast lookups\nCREATE INDEX entity_name IF NOT EXISTS FOR (e:Entity) ON (e.name);\nCREATE INDEX entity_type IF NOT EXISTS FOR (e:Entity) ON (e.type);\nCREATE INDEX fact_confidence IF NOT EXISTS FOR (f:Fact) ON (f.confidence);\nCREATE INDEX goal_status IF NOT EXISTS FOR (g:Goal) ON (g.status);\nCREATE INDEX turn_timestamp IF NOT EXISTS FOR (t:Turn) ON (t.timestamp);\n\n// Full-text search indexes\nCREATE FULLTEXT INDEX entity_search IF NOT EXISTS\nFOR (e:Entity) ON EACH [e.name, e.aliases, e.description];\n\nCREATE FULLTEXT INDEX fact_search IF NOT EXISTS\nFOR (f:Fact) ON EACH [f.claim];\n\n// ============================================\n// VECTOR INDEXES\n// ============================================\n\n// Turn embeddings (episodic memory)\nCREATE VECTOR INDEX turn_embeddings IF NOT EXISTS\nFOR (t:Turn) ON (t.embedding)\nOPTIONS {\n    indexConfig: {\n        `vector.dimensions`: 1536,\n        `vector.similarity_function`: 'cosine'\n    }\n};\n\n// Entity embeddings (semantic memory)\nCREATE VECTOR INDEX entity_embeddings IF NOT EXISTS\nFOR (e:Entity) ON (e.embedding)\nOPTIONS {\n    indexConfig: {\n        `vector.dimensions`: 1536,\n        `vector.similarity_function`: 'cosine'\n    }\n};\n\n// Fact embeddings\nCREATE VECTOR INDEX fact_embeddings IF NOT EXISTS\nFOR (f:Fact) ON (f.embedding)\nOPTIONS {\n    indexConfig: {\n        `vector.dimensions`: 1536,\n        `vector.similarity_function`: 'cosine'\n    }\n};\n\n// Strategy embeddings (procedural memory)\nCREATE VECTOR INDEX strategy_embeddings IF NOT EXISTS\nFOR (s:Strategy) ON (s.embedding)\nOPTIONS {\n    indexConfig: {\n        `vector.dimensions`: 1536,\n        `vector.similarity_function`: 'cosine'\n    }\n};\n</code></pre>"},{"location":"development/memory-setup/#3-initialize-postgresql-schema","title":"3. Initialize PostgreSQL Schema","text":"<p>Connect to PostgreSQL and run the initialization script:</p> <pre><code># Connect to PostgreSQL\ndocker exec -it agent-postgres psql -U agent -d agent_memory\n\n# Or use a SQL file\ndocker exec -i agent-postgres psql -U agent -d agent_memory &lt; init-scripts/01-init.sql\n</code></pre> <p>SQL initialization script:</p> <pre><code>-- Enable pgvector extension\nCREATE EXTENSION IF NOT EXISTS vector;\nCREATE EXTENSION IF NOT EXISTS pg_trgm;  -- For fuzzy text search\n\n-- Conversation logs (append-only time series)\nCREATE TABLE conversation_logs (\n    id BIGSERIAL PRIMARY KEY,\n    conversation_id UUID NOT NULL,\n    turn_index INTEGER NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    role VARCHAR(20) NOT NULL,\n    content TEXT NOT NULL,\n    content_hash VARCHAR(64),\n    token_count INTEGER,\n    model VARCHAR(100),\n    metadata JSONB DEFAULT '{}',\n    embedding vector(1536),\n\n    UNIQUE(conversation_id, turn_index)\n);\n\n-- BRIN index for time-range queries (very efficient for time-series)\nCREATE INDEX idx_logs_timestamp ON conversation_logs USING BRIN (timestamp);\nCREATE INDEX idx_logs_conversation ON conversation_logs (conversation_id);\nCREATE INDEX idx_logs_embedding ON conversation_logs USING ivfflat (embedding vector_cosine_ops)\n    WITH (lists = 100);\n\n-- Memory timeline (unified temporal index)\nCREATE TABLE memory_timeline (\n    id BIGSERIAL PRIMARY KEY,\n    memory_type VARCHAR(50) NOT NULL,\n    neo4j_node_id VARCHAR(100),\n    event_time TIMESTAMPTZ NOT NULL,\n    summary TEXT,\n    embedding vector(1536),\n    importance_score FLOAT DEFAULT 0.5,\n    access_count INTEGER DEFAULT 0,\n    last_accessed TIMESTAMPTZ,\n    archived BOOLEAN DEFAULT FALSE,\n    metadata JSONB DEFAULT '{}'\n);\n\nCREATE INDEX idx_timeline_time ON memory_timeline USING BRIN (event_time);\nCREATE INDEX idx_timeline_type ON memory_timeline (memory_type);\nCREATE INDEX idx_timeline_importance ON memory_timeline (importance_score DESC);\nCREATE INDEX idx_timeline_embedding ON memory_timeline USING ivfflat (embedding vector_cosine_ops)\n    WITH (lists = 100);\n\n-- Tool invocations audit\nCREATE TABLE tool_invocations (\n    id BIGSERIAL PRIMARY KEY,\n    conversation_id UUID NOT NULL,\n    turn_index INTEGER NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    tool_name VARCHAR(100) NOT NULL,\n    tool_input JSONB NOT NULL,\n    tool_output JSONB,\n    success BOOLEAN,\n    latency_ms INTEGER,\n    error_message TEXT\n);\n\nCREATE INDEX idx_tools_conversation ON tool_invocations (conversation_id);\nCREATE INDEX idx_tools_name ON tool_invocations (tool_name);\nCREATE INDEX idx_tools_timestamp ON tool_invocations USING BRIN (timestamp);\n\n-- User preferences and profiles\nCREATE TABLE user_profiles (\n    user_id VARCHAR(100) PRIMARY KEY,\n    created_at TIMESTAMPTZ DEFAULT NOW(),\n    updated_at TIMESTAMPTZ DEFAULT NOW(),\n    preferences JSONB DEFAULT '{}',\n    expertise_areas JSONB DEFAULT '[]',\n    communication_style JSONB DEFAULT '{}',\n    metadata JSONB DEFAULT '{}'\n);\n\n-- Function to update timestamp\nCREATE OR REPLACE FUNCTION update_updated_at()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.updated_at = NOW();\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER user_profiles_updated\n    BEFORE UPDATE ON user_profiles\n    FOR EACH ROW\n    EXECUTE FUNCTION update_updated_at();\n</code></pre>"},{"location":"development/memory-setup/#4-configure-environment-variables","title":"4. Configure Environment Variables","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># Neo4j Configuration\nNEO4J_URI=bolt://localhost:7687\nNEO4J_USER=neo4j\nNEO4J_PASSWORD=your_secure_password\n\n# PostgreSQL Configuration\nPOSTGRES_URI=postgresql://agent:your_secure_password@localhost:5432/agent_memory\n\n# Redis Configuration\nREDIS_URI=redis://localhost:6379\n\n# Embedding Provider\nEMBEDDING_PROVIDER=openai  # or \"local\"\nEMBEDDING_MODEL=text-embedding-3-small\nEMBEDDING_DIMENSIONS=1536\nOPENAI_API_KEY=sk-your-api-key-here\n\n# Local Embedding Model (if using local)\nLOCAL_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5\n\n# Memory Settings\nEPISODIC_RETENTION_DAYS=90\nFACT_CONFIDENCE_THRESHOLD=0.7\nSALIENCE_DECAY_RATE=0.95\nMAX_WORKING_MEMORY_ITEMS=50\n\n# Retrieval Settings\nDEFAULT_TOP_K=10\nRERANKING_ENABLED=true\n</code></pre>"},{"location":"development/memory-setup/#5-install-python-dependencies","title":"5. Install Python Dependencies","text":"<p>Add required dependencies to your project:</p> <pre><code># Using uv (recommended)\nuv add neo4j redis sqlalchemy psycopg2-binary pgvector pydantic-settings\n\n# For OpenAI embeddings\nuv add openai\n\n# For local embeddings\nuv add sentence-transformers\n\n# Optional: for entity extraction\nuv add spacy\npython -m spacy download en_core_web_sm\n</code></pre> <p>Or add to <code>pyproject.toml</code>:</p> <pre><code>[project]\ndependencies = [\n    \"neo4j&gt;=5.15.0\",\n    \"redis&gt;=5.0.0\",\n    \"sqlalchemy&gt;=2.0.0\",\n    \"psycopg2-binary&gt;=2.9.0\",\n    \"pgvector&gt;=0.2.0\",\n    \"pydantic-settings&gt;=2.0.0\",\n    \"openai&gt;=1.0.0\",  # For OpenAI embeddings\n    \"sentence-transformers&gt;=2.2.0\",  # For local embeddings\n]\n</code></pre>"},{"location":"development/memory-setup/#6-verify-installation","title":"6. Verify Installation","text":"<p>Test the memory system:</p> <pre><code>from agentx_ai.kit.agent_memory import AgentMemory, Turn\nfrom uuid import uuid4\n\n# Initialize memory\nmemory = AgentMemory(user_id=\"test_user\", conversation_id=str(uuid4()))\n\n# Store a test turn\nturn = Turn(\n    conversation_id=memory.conversation_id,\n    index=0,\n    role=\"user\",\n    content=\"Hello, this is a test message.\"\n)\nmemory.store_turn(turn)\n\n# Retrieve\ncontext = memory.remember(\"test message\")\nprint(context.to_context_string())\n\n# Clean up\nmemory.close()\n</code></pre>"},{"location":"development/memory-setup/#running-the-consolidation-worker","title":"Running the Consolidation Worker","text":"<p>The background consolidation worker should run as a separate process:</p> <pre><code># Development\npython -m agentx_ai.kit.agent_memory.consolidation.worker\n\n# Production (with supervisor/systemd)\n# See deployment section below\n</code></pre>"},{"location":"development/memory-setup/#docker-compose-configuration","title":"Docker Compose Configuration","text":"<p>Create <code>docker-compose.yml</code> for the memory system databases:</p> <pre><code>version: '3.8'\n\nservices:\n  neo4j:\n    image: neo4j:5.15-community\n    container_name: agent-neo4j\n    ports:\n      - \"7474:7474\"  # Browser\n      - \"7687:7687\"  # Bolt\n    environment:\n      - NEO4J_AUTH=neo4j/your_secure_password\n      - NEO4J_PLUGINS=[\"apoc\"]\n      - NEO4J_apoc_export_file_enabled=true\n      - NEO4J_apoc_import_file_enabled=true\n      - NEO4J_apoc_import_file_use__neo4j__config=true\n      - NEO4J_server_memory_heap_initial__size=512m\n      - NEO4J_server_memory_heap_max__size=2G\n      - NEO4J_server_memory_pagecache_size=1G\n    volumes:\n      - ./data/neo4j/data:/data\n      - ./data/neo4j/logs:/logs\n      - ./data/neo4j/plugins:/plugins\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:7474\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  postgres:\n    image: pgvector/pgvector:pg16\n    container_name: agent-postgres\n    ports:\n      - \"5432:5432\"\n    environment:\n      - POSTGRES_USER=agent\n      - POSTGRES_PASSWORD=your_secure_password\n      - POSTGRES_DB=agent_memory\n    volumes:\n      - ./data/postgres:/var/lib/postgresql/data\n      - ./init-scripts:/docker-entrypoint-initdb.d\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U agent -d agent_memory\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  redis:\n    image: redis:7-alpine\n    container_name: agent-redis\n    ports:\n      - \"6379:6379\"\n    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru\n    volumes:\n      - ./data/redis:/data\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  # Optional: Redis GUI\n  redis-commander:\n    image: rediscommander/redis-commander:latest\n    container_name: agent-redis-gui\n    ports:\n      - \"8081:8081\"\n    environment:\n      - REDIS_HOSTS=local:redis:6379\n    depends_on:\n      - redis\n</code></pre>"},{"location":"development/memory-setup/#development-workflow","title":"Development Workflow","text":""},{"location":"development/memory-setup/#testing-memory-operations","title":"Testing Memory Operations","text":"<pre><code># Test episodic memory\nfrom agentx_ai.kit.agent_memory import AgentMemory, Turn\nfrom uuid import uuid4\n\nmemory = AgentMemory(user_id=\"dev_user\", conversation_id=str(uuid4()))\n\n# Add multiple turns\nfor i, content in enumerate([\"Hello\", \"How are you?\", \"Tell me about Python\"]):\n    turn = Turn(\n        conversation_id=memory.conversation_id,\n        index=i,\n        role=\"user\" if i % 2 == 0 else \"assistant\",\n        content=content\n    )\n    memory.store_turn(turn)\n\n# Test retrieval\ncontext = memory.remember(\"Python programming\", top_k=5)\nprint(f\"Found {len(context.relevant_turns)} relevant turns\")\n\n# Test semantic memory\nfrom agentx_ai.kit.agent_memory import Entity\n\nentity = Entity(\n    name=\"Python\",\n    type=\"ProgrammingLanguage\",\n    description=\"High-level programming language\"\n)\nmemory.upsert_entity(entity)\n\n# Test procedural memory\nmemory.record_tool_usage(\n    tool_name=\"code_interpreter\",\n    tool_input={\"code\": \"print('hello')\"},\n    tool_output={\"result\": \"hello\"},\n    success=True,\n    latency_ms=150\n)\n</code></pre>"},{"location":"development/memory-setup/#monitoring","title":"Monitoring","text":"<p>Check database health:</p> <pre><code># Neo4j stats\ndocker exec agent-neo4j cypher-shell -u neo4j -p password \"CALL dbms.listConfig() YIELD name, value WHERE name STARTS WITH 'dbms.memory' RETURN name, value\"\n\n# PostgreSQL stats\ndocker exec agent-postgres psql -U agent -d agent_memory -c \"SELECT schemaname, tablename, pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size FROM pg_tables WHERE schemaname NOT IN ('pg_catalog', 'information_schema') ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\"\n\n# Redis stats\ndocker exec agent-redis redis-cli INFO memory\n</code></pre>"},{"location":"development/memory-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/memory-setup/#neo4j-connection-issues","title":"Neo4j Connection Issues","text":"<pre><code># Check if Neo4j is running\ndocker ps | grep neo4j\n\n# Check logs\ndocker logs agent-neo4j\n\n# Test connection\ndocker exec agent-neo4j cypher-shell -u neo4j -p password \"RETURN 'Connected' as status\"\n</code></pre>"},{"location":"development/memory-setup/#postgresql-connection-issues","title":"PostgreSQL Connection Issues","text":"<pre><code># Check if PostgreSQL is running\ndocker ps | grep postgres\n\n# Check logs\ndocker logs agent-postgres\n\n# Test connection\ndocker exec agent-postgres pg_isready -U agent -d agent_memory\n</code></pre>"},{"location":"development/memory-setup/#redis-connection-issues","title":"Redis Connection Issues","text":"<pre><code># Check if Redis is running\ndocker ps | grep redis\n\n# Test connection\ndocker exec agent-redis redis-cli ping\n</code></pre>"},{"location":"development/memory-setup/#vector-index-issues","title":"Vector Index Issues","text":"<p>If vector searches are slow:</p> <pre><code>// Check vector index status\nSHOW INDEXES YIELD name, type, state WHERE type = \"VECTOR\";\n\n// Rebuild vector index if needed\nDROP INDEX turn_embeddings;\nCREATE VECTOR INDEX turn_embeddings FOR (t:Turn) ON (t.embedding)\nOPTIONS {indexConfig: {`vector.dimensions`: 1536, `vector.similarity_function`: 'cosine'}};\n</code></pre>"},{"location":"development/memory-setup/#production-deployment","title":"Production Deployment","text":""},{"location":"development/memory-setup/#using-supervisor","title":"Using Supervisor","text":"<p>Create <code>/etc/supervisor/conf.d/agentx-memory-worker.conf</code>:</p> <pre><code>[program:agentx-memory-worker]\ncommand=/path/to/venv/bin/python -m agentx_ai.kit.agent_memory.consolidation.worker\ndirectory=/path/to/agentx-source\nuser=agentx\nautostart=true\nautorestart=true\nstderr_logfile=/var/log/agentx/memory-worker.err.log\nstdout_logfile=/var/log/agentx/memory-worker.out.log\nenvironment=PATH=\"/path/to/venv/bin\"\n</code></pre>"},{"location":"development/memory-setup/#using-systemd","title":"Using Systemd","text":"<p>Create <code>/etc/systemd/system/agentx-memory-worker.service</code>:</p> <pre><code>[Unit]\nDescription=AgentX Memory Consolidation Worker\nAfter=network.target\n\n[Service]\nType=simple\nUser=agentx\nWorkingDirectory=/path/to/agentx-source\nEnvironment=\"PATH=/path/to/venv/bin\"\nExecStart=/path/to/venv/bin/python -m agentx_ai.kit.agent_memory.consolidation.worker\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Then:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable agentx-memory-worker\nsudo systemctl start agentx-memory-worker\nsudo systemctl status agentx-memory-worker\n</code></pre>"},{"location":"development/memory-setup/#next-steps","title":"Next Steps","text":"<ol> <li>Implement entity extraction (see <code>extraction/entities.py</code>)</li> <li>Implement fact extraction (see <code>extraction/facts.py</code>)</li> <li>Configure monitoring and alerting</li> <li>Set up backup procedures for databases</li> <li>Tune database parameters for your workload</li> </ol>"},{"location":"development/memory-setup/#related-documentation","title":"Related Documentation","text":"<ul> <li>Memory System Overview</li> <li>Memory Architecture</li> <li>Database Stack</li> </ul>"},{"location":"development/setup/","title":"Development Setup","text":"<p>Set up your development environment for contributing to AgentX.</p>"},{"location":"development/setup/#prerequisites","title":"Prerequisites","text":"<p>Install required tools:</p> <ul> <li>Python 3.10+ with uv</li> <li>Node.js 18+ with bun</li> <li>Docker &amp; Docker Compose</li> <li>Task runner</li> <li>Git</li> </ul> <p>See Installation for details.</p>"},{"location":"development/setup/#initial-setup","title":"Initial Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/yourusername/agentx-source.git\ncd agentx-source\n\n# Install dependencies\ntask install\n\n# Initialize databases\ntask db:init\n\n# Start development environment\ntask dev\n</code></pre>"},{"location":"development/setup/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create feature branch</li> <li>Make changes with hot reload</li> <li>Run tests</li> <li>Commit and push</li> <li>Create pull request</li> </ol> <p>See Contributing Guide for detailed workflow.</p>"},{"location":"development/tasks/","title":"Task Commands","text":"<p>Complete reference for Taskfile automation commands.</p>"},{"location":"development/tasks/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"development/tasks/#install-all-dependencies","title":"Install All Dependencies","text":"<pre><code>task install\n</code></pre> <p>Runs <code>uv sync</code> and <code>bun install</code> to install all dependencies.</p>"},{"location":"development/tasks/#initialize-databases","title":"Initialize Databases","text":"<pre><code>task db:init\n</code></pre> <p>Creates data directories for Neo4j, PostgreSQL, and Redis.</p>"},{"location":"development/tasks/#development","title":"Development","text":""},{"location":"development/tasks/#start-development-environment","title":"Start Development Environment","text":"<pre><code>task dev\n</code></pre> <p>Starts all services:</p> <ul> <li>Docker containers (Neo4j, Postgres, Redis)</li> <li>Django API on port 12319</li> <li>Tauri development window</li> </ul>"},{"location":"development/tasks/#start-services-only","title":"Start Services Only","text":"<pre><code>task runners\n</code></pre> <p>Starts Docker containers without API or client.</p>"},{"location":"development/tasks/#stop-all-services","title":"Stop All Services","text":"<pre><code>task teardown\n</code></pre> <p>Stops and removes Docker containers.</p>"},{"location":"development/tasks/#pre-launch-check","title":"Pre-Launch Check","text":"<pre><code>task pre-launch-check\n</code></pre> <p>Validates that all required directories and dependencies exist.</p>"},{"location":"development/tasks/#api-commands","title":"API Commands","text":""},{"location":"development/tasks/#run-django-server","title":"Run Django Server","text":"<pre><code>task api:runserver\n</code></pre> <p>Starts Django development server on <code>http://127.0.0.1:12319</code>.</p>"},{"location":"development/tasks/#django-shell","title":"Django Shell","text":"<pre><code>task api:shell\n</code></pre> <p>Opens Django interactive shell.</p>"},{"location":"development/tasks/#database-migrations","title":"Database Migrations","text":"<p>Create migration files: <pre><code>task api:makemigrations\n</code></pre></p> <p>Apply migrations: <pre><code>task api:migrate\n</code></pre></p>"},{"location":"development/tasks/#client-commands","title":"Client Commands","text":""},{"location":"development/tasks/#start-tauri-dev-mode","title":"Start Tauri Dev Mode","text":"<pre><code>task client:dev\n</code></pre> <p>Launches Tauri window with hot reload.</p>"},{"location":"development/tasks/#build-client","title":"Build Client","text":"<pre><code>task client:build\n</code></pre> <p>Builds production client bundle.</p>"},{"location":"development/tasks/#start-production-client","title":"Start Production Client","text":"<pre><code>task client:start\n</code></pre> <p>Runs production build (requires <code>client:build</code> first).</p>"},{"location":"development/tasks/#database-management","title":"Database Management","text":""},{"location":"development/tasks/#postgresql","title":"PostgreSQL","text":"<p>Open PostgreSQL shell: <pre><code>task db:shell:postgres\n</code></pre></p> <p>Create backup: <pre><code>task db:backup:postgres\n</code></pre></p> <p>Backups saved to <code>./backups/postgres_YYYYMMDD_HHMMSS.sql</code></p> <p>Restore from backup: <pre><code>task db:restore:postgres BACKUP_FILE=backups/postgres_20231127_120000.sql\n</code></pre></p>"},{"location":"development/tasks/#redis","title":"Redis","text":"<p>Open Redis CLI: <pre><code>task db:shell:redis\n</code></pre></p>"},{"location":"development/tasks/#neo4j","title":"Neo4j","text":"<p>Neo4j Browser: http://localhost:7474</p>"},{"location":"development/tasks/#volume-migration","title":"Volume Migration","text":"<p>Migrate all databases from Docker volumes to local bind mounts: <pre><code>task db:migrate-volumes\n</code></pre></p> <p>Migrate individual databases: <pre><code>task db:migrate-volumes:neo4j\ntask db:migrate-volumes:postgres\ntask db:migrate-volumes:redis\n</code></pre></p> <p>These commands copy data from Docker volumes to <code>./data/</code> directories.</p>"},{"location":"development/tasks/#clean-all-data","title":"Clean All Data","text":"<p>Destructive Operation</p> <p>This permanently deletes all database data!</p> <pre><code>task db:clean\n</code></pre> <p>You will be prompted for confirmation.</p>"},{"location":"development/tasks/#testing","title":"Testing","text":""},{"location":"development/tasks/#run-all-tests","title":"Run All Tests","text":"<pre><code>task test\n</code></pre> <p>Runs Django test suite.</p>"},{"location":"development/tasks/#run-specific-test","title":"Run Specific Test","text":"<pre><code>uv run python api/manage.py test agentx_ai.TranslationKitTest\n</code></pre>"},{"location":"development/tasks/#run-single-test-method","title":"Run Single Test Method","text":"<pre><code>uv run python api/manage.py test agentx_ai.TranslationKitTest.test_translate_to_french\n</code></pre>"},{"location":"development/tasks/#documentation","title":"Documentation","text":""},{"location":"development/tasks/#serve-documentation-locally","title":"Serve Documentation Locally","text":"<pre><code>task docs:serve\n</code></pre> <p>Opens documentation at http://127.0.0.1:8000 with live reload.</p>"},{"location":"development/tasks/#build-documentation","title":"Build Documentation","text":"<pre><code>task docs:build\n</code></pre> <p>Builds static documentation site to <code>site/</code> directory.</p>"},{"location":"development/tasks/#deploy-documentation","title":"Deploy Documentation","text":"<pre><code>task docs:deploy\n</code></pre> <p>Deploys documentation to GitHub Pages (requires git repository setup).</p>"},{"location":"development/tasks/#utility-commands","title":"Utility Commands","text":""},{"location":"development/tasks/#default-task","title":"Default Task","text":"<pre><code>task\n</code></pre> <p>Runs sanity check and prompts to start development environment.</p>"},{"location":"development/tasks/#list-all-tasks","title":"List All Tasks","text":"<pre><code>task --list\n</code></pre> <p>Shows all available tasks with descriptions.</p>"},{"location":"development/tasks/#task-help","title":"Task Help","text":"<pre><code>task --help\n</code></pre> <p>Displays Taskfile help information.</p>"},{"location":"development/tasks/#task-dependencies","title":"Task Dependencies","text":"<p>Some tasks automatically run prerequisites:</p> <ul> <li><code>task dev</code> \u2192 runs <code>task runners</code> first</li> <li><code>task client:start</code> \u2192 runs <code>task client:build</code> first</li> <li><code>task runners</code> \u2192 runs <code>task pre-launch-check</code> first</li> </ul>"},{"location":"development/tasks/#environment-specific-tasks","title":"Environment-Specific Tasks","text":""},{"location":"development/tasks/#development_1","title":"Development","text":"<pre><code># Quick iteration cycle\ntask dev              # Start everything\n# Make changes...\ntask test             # Verify changes\ntask teardown         # Stop when done\n</code></pre>"},{"location":"development/tasks/#testing_1","title":"Testing","text":"<pre><code># Run specific tests during development\nuv run python api/manage.py test agentx_ai --keepdb\n</code></pre>"},{"location":"development/tasks/#production-build","title":"Production Build","text":"<pre><code>task client:build     # Build optimized client\ntask api:migrate      # Ensure migrations are applied\n</code></pre>"},{"location":"development/tasks/#custom-task-variables","title":"Custom Task Variables","text":"<p>Some tasks accept variables:</p>"},{"location":"development/tasks/#postgres-restore","title":"Postgres Restore","text":"<pre><code>task db:restore:postgres BACKUP_FILE=path/to/backup.sql\n</code></pre>"},{"location":"development/tasks/#debugging-tasks","title":"Debugging Tasks","text":""},{"location":"development/tasks/#verbose-output","title":"Verbose Output","text":"<pre><code>task --verbose dev\n</code></pre>"},{"location":"development/tasks/#dry-run","title":"Dry Run","text":"<pre><code>task --dry dev\n</code></pre> <p>Shows what would be executed without running commands.</p>"},{"location":"development/tasks/#tips-tricks","title":"Tips &amp; Tricks","text":""},{"location":"development/tasks/#run-multiple-commands","title":"Run Multiple Commands","text":"<pre><code>task install &amp;&amp; task db:init &amp;&amp; task dev\n</code></pre>"},{"location":"development/tasks/#background-execution","title":"Background Execution","text":"<pre><code>task runners &amp;  # Start services in background\n</code></pre>"},{"location":"development/tasks/#watch-mode","title":"Watch Mode","text":"<pre><code># API auto-reloads on file changes\ntask api:runserver\n\n# Client has HMR enabled\ntask client:dev\n</code></pre>"},{"location":"development/tasks/#quick-database-reset","title":"Quick Database Reset","text":"<pre><code>task teardown &amp;&amp; task db:clean &amp;&amp; task db:init &amp;&amp; task runners\n</code></pre> <p>Warning</p> <p>This deletes all data!</p>"},{"location":"development/tasks/#next-steps","title":"Next Steps","text":"<ul> <li>Development Setup - Configure your environment</li> <li>Testing Guide - Write and run tests</li> <li>Contributing - Contribution workflow</li> </ul>"},{"location":"development/testing/","title":"Testing","text":"<p>Guide to writing and running tests for AgentX.</p>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#all-tests","title":"All Tests","text":"<pre><code># Run all tests\ntask test\n\n# Or directly with Django\nuv run python api/manage.py test agentx_ai\n</code></pre>"},{"location":"development/testing/#specific-tests","title":"Specific Tests","text":"<pre><code># Run a specific test class\nuv run python api/manage.py test agentx_ai.tests.TranslationKitTest\n\n# Run a single test method\nuv run python api/manage.py test agentx_ai.tests.TranslationKitTest.test_translate_to_french\n\n# Run health check tests\nuv run python api/manage.py test agentx_ai.tests.HealthCheckTest\n</code></pre>"},{"location":"development/testing/#verbose-output","title":"Verbose Output","text":"<pre><code># Show detailed output\nuv run python api/manage.py test agentx_ai --verbosity=2\n</code></pre>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<p>Tests are located in <code>api/agentx_ai/tests.py</code> and organized by feature:</p> <pre><code>api/agentx_ai/tests.py\n\u251c\u2500\u2500 TranslationKitTest      # Translation system tests\n\u251c\u2500\u2500 HealthCheckTest         # API health endpoint tests\n\u2514\u2500\u2500 LanguageDetectionTest   # Language detection tests\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#basic-test-case","title":"Basic Test Case","text":"<pre><code>from django.test import TestCase\n\nclass MyFeatureTest(TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        pass\n\n    def test_basic_functionality(self):\n        \"\"\"Test description here.\"\"\"\n        result = my_function()\n        self.assertEqual(result, expected_value)\n\n    def tearDown(self):\n        \"\"\"Clean up after tests.\"\"\"\n        pass\n</code></pre>"},{"location":"development/testing/#api-endpoint-tests","title":"API Endpoint Tests","text":"<pre><code>from django.test import TestCase, Client\n\nclass APIEndpointTest(TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_health_endpoint(self):\n        response = self.client.get('/api/health')\n        self.assertEqual(response.status_code, 200)\n        data = response.json()\n        self.assertEqual(data['status'], 'healthy')\n\n    def test_translation_endpoint(self):\n        response = self.client.post(\n            '/api/tools/translate',\n            data={'text': 'Hello', 'targetLanguage': 'fra_Latn'},\n            content_type='application/json'\n        )\n        self.assertEqual(response.status_code, 200)\n</code></pre>"},{"location":"development/testing/#skipping-tests","title":"Skipping Tests","text":"<p>Use <code>@unittest.skip</code> for tests that aren't ready:</p> <pre><code>import unittest\n\nclass MyTest(TestCase):\n    @unittest.skip(\"Requires model download - slow\")\n    def test_slow_model_operation(self):\n        pass\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Test individual functions and classes in isolation:</p> <ul> <li>Translation kit methods</li> <li>Language lexicon conversions</li> <li>Memory utilities</li> </ul>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Test component interactions:</p> <ul> <li>API endpoint flows</li> <li>Database connections</li> <li>MCP client operations</li> </ul>"},{"location":"development/testing/#end-to-end-tests","title":"End-to-End Tests","text":"<p>Test complete user flows (future):</p> <ul> <li>Client \u2192 API \u2192 Model \u2192 Response</li> <li>Full translation workflow</li> </ul>"},{"location":"development/testing/#database-tests","title":"Database Tests","text":"<p>For tests requiring database services:</p> <pre><code>from django.test import TestCase\nfrom agentx_ai.kit.memory_utils import check_memory_health\n\nclass MemoryTest(TestCase):\n    def test_memory_health(self):\n        \"\"\"Requires Docker services running.\"\"\"\n        health = check_memory_health()\n        self.assertIn('neo4j', health)\n        self.assertIn('postgres', health)\n        self.assertIn('redis', health)\n</code></pre> <p>Docker Services</p> <p>Run <code>task runners</code> before executing database tests.</p>"},{"location":"development/testing/#coverage","title":"Coverage","text":"<p>To check test coverage (requires <code>coverage</code> package):</p> <pre><code># Run with coverage\nuv run coverage run --source=agentx_ai api/manage.py test agentx_ai\n\n# View report\nuv run coverage report\n\n# Generate HTML report\nuv run coverage html\nopen htmlcov/index.html\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":"<ol> <li>Descriptive names: <code>test_translate_english_to_french</code> not <code>test_1</code></li> <li>One assertion focus: Each test should verify one behavior</li> <li>Independent tests: Tests shouldn't depend on each other</li> <li>Fast tests: Mock external services when possible</li> <li>Clean up: Use <code>tearDown()</code> to reset state</li> </ol>"},{"location":"development/testing/#see-also","title":"See Also","text":"<ul> <li>Django Testing Documentation</li> <li>Contributing Guide</li> </ul>"},{"location":"features/chat/","title":"Chat Interface","text":"<p>AI-powered conversational interface with session management and memory integration.</p>"},{"location":"features/chat/#overview","title":"Overview","text":"<p>The Chat interface provides multi-turn conversations with the AgentX agent, featuring:</p> <ul> <li>Session Management: Persistent conversation sessions</li> <li>Context Awareness: Sliding window context with summarization</li> <li>Memory Integration: Access to episodic and semantic memory</li> <li>Reasoning Traces: Visibility into agent's thought process</li> </ul>"},{"location":"features/chat/#usage","title":"Usage","text":""},{"location":"features/chat/#starting-a-conversation","title":"Starting a Conversation","text":"<p>Send a message to begin or continue a chat session:</p> <pre><code>curl -X POST http://localhost:12319/api/agent/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\": \"Hello, what can you help me with?\"}'\n</code></pre>"},{"location":"features/chat/#session-continuity","title":"Session Continuity","text":"<p>Include a <code>session_id</code> to continue an existing conversation:</p> <pre><code>curl -X POST http://localhost:12319/api/agent/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"message\": \"Tell me more about that\",\n    \"session_id\": \"550e8400-e29b-41d4-a716-446655440000\"\n  }'\n</code></pre>"},{"location":"features/chat/#features","title":"Features","text":""},{"location":"features/chat/#context-management","title":"Context Management","text":"<p>The agent automatically manages conversation context:</p> <ul> <li>Token Estimation: Tracks context window usage</li> <li>Sliding Window: Keeps recent messages in full detail</li> <li>Summarization: Compresses older context when needed</li> </ul>"},{"location":"features/chat/#reasoning-strategies","title":"Reasoning Strategies","text":"<p>Chat interactions can use various reasoning patterns:</p> Strategy Use Case Chain-of-Thought Step-by-step reasoning Tree-of-Thought Exploring multiple approaches ReAct Tasks requiring tool use Reflection Self-critique and improvement"},{"location":"features/chat/#tool-integration","title":"Tool Integration","text":"<p>The chat interface can access tools from connected MCP servers:</p> <ul> <li>File system operations</li> <li>Web search</li> <li>Database queries</li> <li>Custom tools</li> </ul>"},{"location":"features/chat/#client-integration","title":"Client Integration","text":"<p>The Tauri client provides a dedicated Chat tab with:</p> <ul> <li>Message history display</li> <li>Session selector</li> <li>Reasoning trace viewer</li> <li>Tool usage visualization</li> </ul>"},{"location":"features/chat/#api-reference","title":"API Reference","text":"<p>See Agent Endpoints for full API documentation.</p>"},{"location":"features/memory/","title":"Memory System","text":"<p>AgentX features a sophisticated cognitive memory system inspired by human memory architecture, enabling the AI to remember past conversations, learn from interactions, and build a knowledge graph over time.</p>"},{"location":"features/memory/#overview","title":"Overview","text":"<p>The memory system provides four types of memory:</p> Memory Type Purpose Storage Retrieval Working Current conversation, active goals Redis Direct lookup Episodic Past conversations, events Neo4j + PostgreSQL Vector similarity + temporal Semantic Facts, entities, concepts Neo4j graph Graph traversal + vector Procedural Successful strategies, tool patterns Neo4j graph Task-type matching"},{"location":"features/memory/#design-principles","title":"Design Principles","text":"<ul> <li>Extensibility: Easy to add new memory types, stores, or extraction methods</li> <li>Transparency: All memory operations traceable per session/conversation</li> <li>Auditability: Full query trace and operation audit trail in PostgreSQL</li> <li>Channel Scoping: Memory organized into channels for project-level organization</li> </ul>"},{"location":"features/memory/#memory-channels","title":"Memory Channels","text":"<p>Memory is organized into channels \u2014 traceable scopes that group related memories:</p> Channel Description Examples <code>_global</code> Default channel, user-wide memory Preferences, communication style, general facts <code>&lt;project&gt;</code> Project-specific memory containers <code>my-rust-project</code>, <code>thesis-research</code>, <code>work-api</code> <p>Key behaviors: - Retrieval queries both the active channel AND <code>_global</code>, merging results - Channels are traceable scopes, not isolation boundaries - Cross-channel operations are logged in the audit trail - Prominent project facts can be promoted to <code>_global</code> based on confidence/frequency thresholds</p> <pre><code># Using channels\nmemory = AgentMemory(user_id=\"user123\", channel=\"my-rust-project\")\n\n# Retrieval automatically merges project + global memories\ncontext = memory.remember(\"What error handling pattern should I use?\")\n# Returns: project-specific Rust patterns + global user preferences\n</code></pre>"},{"location":"features/memory/#key-features","title":"Key Features","text":""},{"location":"features/memory/#episodic-memory","title":"Episodic Memory","text":"<ul> <li>Stores complete conversation history with turns</li> <li>Vector-based semantic search across conversations</li> <li>Temporal filtering and recency boosting</li> <li>Automatic consolidation to semantic memory</li> </ul>"},{"location":"features/memory/#semantic-memory","title":"Semantic Memory","text":"<ul> <li>Entity recognition and tracking (people, organizations, concepts)</li> <li>Fact extraction with confidence scores</li> <li>Knowledge graph with relationships</li> <li>Entity salience scoring and decay</li> </ul>"},{"location":"features/memory/#procedural-memory","title":"Procedural Memory","text":"<ul> <li>Records tool usage patterns</li> <li>Learns successful strategies for tasks</li> <li>Reinforcement learning from outcomes</li> <li>Tool performance analytics</li> </ul>"},{"location":"features/memory/#working-memory","title":"Working Memory","text":"<ul> <li>Redis-based fast access for current session</li> <li>Maintains recent conversation context</li> <li>Active goal tracking</li> <li>Chain-of-thought reasoning steps</li> </ul>"},{"location":"features/memory/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         AGENT RUNTIME                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502   LLM API   \u2502  \u2502  Tool Exec  \u2502  \u2502  Planning   \u2502                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502         \u2502                \u2502                \u2502                          \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                          \u25bc                                           \u2502\n\u2502                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502\n\u2502                 \u2502 Memory Interface\u2502                                  \u2502\n\u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u25bc                  \u25bc                  \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Neo4j      \u2502  \u2502  PostgreSQL   \u2502  \u2502    Redis      \u2502\n\u2502 Graph+Vector  \u2502  \u2502  pgvector     \u2502  \u2502 Working Mem   \u2502\n\u2502               \u2502  \u2502  Time-series  \u2502  \u2502 Cache Layer   \u2502\n\u2502 - Semantic    \u2502  \u2502               \u2502  \u2502               \u2502\n\u2502 - Episodic    \u2502  \u2502 - Raw logs    \u2502  \u2502 - Hot queries \u2502\n\u2502 - Procedural  \u2502  \u2502 - Audit trail \u2502  \u2502 - Session     \u2502\n\u2502 - Entities    \u2502  \u2502 - Timeline    \u2502  \u2502 - Rate limits \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"features/memory/#technology-stack","title":"Technology Stack","text":"Component Technology Purpose Graph Database Neo4j 5.15+ Knowledge graph, vector search, relationships Relational DB PostgreSQL 16+ Logs, audit, time-series, backup vectors Vector Extension pgvector 0.7+ ANN search in PostgreSQL Cache Redis 7+ Working memory, session state Embeddings OpenAI / Local text-embedding-3-small or nomic-embed-text"},{"location":"features/memory/#usage","title":"Usage","text":""},{"location":"features/memory/#basic-usage","title":"Basic Usage","text":"<pre><code>from agentx_ai.kit.agent_memory import AgentMemory, Turn\n\n# Initialize memory for a user\nmemory = AgentMemory(user_id=\"user123\", conversation_id=\"conv456\")\n\n# Store a conversation turn\nturn = Turn(\n    conversation_id=\"conv456\",\n    index=0,\n    role=\"user\",\n    content=\"What's the weather like today?\"\n)\nmemory.store_turn(turn)\n\n# Retrieve relevant memories\ncontext = memory.remember(\"What did we discuss about weather?\")\nprint(context.to_context_string())\n\n# Learn a new fact\nmemory.learn_fact(\n    claim=\"User prefers concise responses\",\n    source=\"inferred\",\n    confidence=0.8\n)\n\n# Track a goal\nfrom agentx_ai.kit.agent_memory import Goal\ngoal = Goal(\n    description=\"Help user plan vacation\",\n    priority=4\n)\nmemory.add_goal(goal)\n\n# Complete a goal\nmemory.complete_goal(goal.id, status=\"completed\", result=\"Vacation itinerary created\")\n\n# Or mark as abandoned/blocked\nmemory.complete_goal(goal.id, status=\"abandoned\", result=\"User cancelled request\")\n\n# Record tool usage for learning\nmemory.record_tool_usage(\n    tool_name=\"web_search\",\n    tool_input={\"query\": \"weather\"},\n    tool_output={\"results\": [...]},\n    success=True,\n    latency_ms=250\n)\n</code></pre>"},{"location":"features/memory/#advanced-retrieval","title":"Advanced Retrieval","text":"<pre><code># Retrieve with filters\ncontext = memory.remember(\n    query=\"Python programming discussion\",\n    top_k=15,\n    include_episodic=True,\n    include_semantic=True,\n    include_procedural=True,\n    time_window_hours=72  # Last 3 days only\n)\n\n# Find what worked for similar tasks\nstrategies = memory.what_worked_for(\"data analysis task\")\nfor strategy in strategies:\n    print(f\"Strategy: {strategy.description}\")\n    print(f\"Tools: {strategy.tool_sequence}\")\n    print(f\"Success rate: {strategy.success_count / max(1, strategy.success_count + strategy.failure_count)}\")\n\n# Get active goals\ngoals = memory.get_active_goals()\nfor goal in goals:\n    print(f\"[P{goal.priority}] {goal.description} - {goal.status}\")\n</code></pre>"},{"location":"features/memory/#background-processing","title":"Background Processing","text":"<p>The system includes a consolidation worker that runs background jobs:</p> <ul> <li>Consolidation (every 15 min): Extracts entities and facts from recent conversations</li> <li>Pattern Detection (hourly): Learns successful strategies from conversation outcomes</li> <li>Memory Decay (daily): Applies time-based decay to salience scores</li> <li>Cleanup (daily): Archives old conversations and removes low-salience entities</li> </ul> <p>To run the worker:</p> <pre><code>python -m agentx_ai.kit.agent_memory.consolidation.worker\n</code></pre>"},{"location":"features/memory/#configuration","title":"Configuration","text":"<p>Configure the memory system via environment variables:</p> <pre><code># Neo4j\nNEO4J_URI=bolt://localhost:7687\nNEO4J_USER=neo4j\nNEO4J_PASSWORD=your_password\n\n# PostgreSQL\nPOSTGRES_URI=postgresql://agent:password@localhost:5432/agent_memory\n\n# Redis\nREDIS_URI=redis://localhost:6379\n\n# Embeddings\nEMBEDDING_PROVIDER=openai  # or \"local\"\nEMBEDDING_MODEL=text-embedding-3-small\nOPENAI_API_KEY=sk-...\n\n# Memory settings\nEPISODIC_RETENTION_DAYS=90\nFACT_CONFIDENCE_THRESHOLD=0.7\nSALIENCE_DECAY_RATE=0.95\nMAX_WORKING_MEMORY_ITEMS=50\n</code></pre>"},{"location":"features/memory/#audit-logging","title":"Audit Logging","text":"<p>All memory operations are logged to a partitioned PostgreSQL table for traceability:</p> Log Level What's Logged <code>off</code> No audit logging <code>writes</code> Store, update, delete operations (default) <code>reads</code> All reads and writes with query details <code>verbose</code> Full traces including payloads <p>Logged information includes: - Operation type, timestamp, user/session/conversation IDs - Source channel and target channels (for cross-channel operations) - Query text and result count for retrievals - Latency per operation - Promotion tracking (when facts are promoted from project to <code>_global</code>) - Configuration snapshot (active thresholds at time of operation)</p> <pre><code># Configure audit logging\nAUDIT_LOG_LEVEL=writes\nAUDIT_RETENTION_DAYS=30\n</code></pre>"},{"location":"features/memory/#database-setup","title":"Database Setup","text":"<p>Initialize the memory system schemas before first use:</p> <pre><code># Start database services\ntask db:up\n\n# Initialize all schemas (Neo4j, PostgreSQL, Redis)\ntask db:init:schemas\n\n# Or verify existing schemas\ntask db:verify:schemas\n</code></pre> <p>This creates: - Neo4j: Vector indexes, uniqueness constraints, channel indexes - PostgreSQL: Memory tables with channel columns, partitioned audit log - Redis: Verifies connectivity and documents key patterns</p>"},{"location":"features/memory/#status","title":"Status","text":"<p>The memory system implementation is complete and syntax-error free. Current status:</p> <ul> <li>\u2705 Core memory interfaces implemented</li> <li>\u2705 Episodic, semantic, procedural, and working memory modules</li> <li>\u2705 Multi-strategy retrieval engine</li> <li>\u2705 Background consolidation worker</li> <li>\u2705 Memory decay and cleanup utilities</li> <li>\u2705 Database schema initialization (<code>task db:init:schemas</code>)</li> <li>\u2705 Channel scoping support in all schemas</li> <li>\u2705 Partitioned audit log table (daily partitions)</li> <li>\u2705 Agent core integration (memory wired into chat/run flows)</li> <li>\u2705 Goal tracking integrated with TaskPlanner (add_goal, get_goal, complete_goal)</li> <li>\u2705 Consolidation uses AgentMemory interface (upsert_entity, learn_fact)</li> <li>\u2705 Entity/fact/relationship extraction (LLM-based via ExtractionService)</li> <li>\u23f3 Audit logger instrumentation</li> </ul>"},{"location":"features/memory/#related-documentation","title":"Related Documentation","text":"<ul> <li>Memory System Architecture - Detailed technical architecture</li> <li>Database Stack - Infrastructure details</li> </ul>"},{"location":"features/translation/","title":"Translation System","text":"<p>Multi-level language detection and translation system.</p>"},{"location":"features/translation/#overview","title":"Overview","text":"<p>Two-level architecture:</p> <ul> <li>Level I: Fast detection (~20 languages)</li> <li>Level II: Comprehensive translation (200+ languages)</li> </ul>"},{"location":"features/translation/#models","title":"Models","text":"<ul> <li>Detection: <code>eleldar/language-detection</code></li> <li>Translation: <code>facebook/m2m100_418M</code> or <code>facebook/nllb-200-distilled-600M</code></li> </ul>"},{"location":"features/translation/#api-usage","title":"API Usage","text":"<pre><code>curl -X POST http://localhost:12319/api/translate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\": \"Hello\", \"target_language\": \"fr\"}'\n</code></pre> <p>See API Endpoints for details.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Configure AgentX for your environment.</p>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file in the project root (optional):</p> <pre><code># Django Settings\nDJANGO_SECRET_KEY=your-secret-key-here\nDJANGO_DEBUG=True\nDJANGO_ALLOWED_HOSTS=localhost,127.0.0.1\n\n# Database Connections\nNEO4J_URI=bolt://localhost:7687\nNEO4J_USER=neo4j\nNEO4J_PASSWORD=your_secure_password\n\nPOSTGRES_URI=postgresql://agent:your_secure_password@localhost:5432/agent_memory\nREDIS_URI=redis://localhost:6379\n\n# API Settings\nAPI_PORT=12319\n</code></pre>"},{"location":"getting-started/configuration/#database-configuration","title":"Database Configuration","text":""},{"location":"getting-started/configuration/#neo4j","title":"Neo4j","text":"<p>Edit <code>docker-compose.yml</code> to customize Neo4j settings:</p> <pre><code>services:\n  neo4j:\n    environment:\n      - NEO4J_AUTH=neo4j/your_secure_password\n      - NEO4J_server_memory_heap_max__size=2G\n      - NEO4J_server_memory_pagecache_size=1G\n</code></pre>"},{"location":"getting-started/configuration/#postgresql","title":"PostgreSQL","text":"<p>Configure PostgreSQL in <code>docker-compose.yml</code>:</p> <pre><code>services:\n  postgres:\n    environment:\n      - POSTGRES_USER=agent\n      - POSTGRES_PASSWORD=your_secure_password\n      - POSTGRES_DB=agent_memory\n</code></pre>"},{"location":"getting-started/configuration/#redis","title":"Redis","text":"<p>Adjust Redis memory limits:</p> <pre><code>services:\n  redis:\n    command: redis-server --maxmemory 512mb --maxmemory-policy allkeys-lru\n</code></pre>"},{"location":"getting-started/configuration/#django-settings","title":"Django Settings","text":"<p>Located in <code>api/agentx_api/settings.py</code>:</p>"},{"location":"getting-started/configuration/#database-backend","title":"Database Backend","text":"<p>Currently using SQLite for Django ORM:</p> <pre><code>DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.sqlite3',\n    }\n}\n</code></pre> <p>To switch to PostgreSQL:</p> <pre><code>DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'NAME': 'agent_memory',\n        'USER': 'agent',\n        'PASSWORD': 'your_secure_password',\n        'HOST': 'localhost',\n        'PORT': '5432',\n    }\n}\n</code></pre>"},{"location":"getting-started/configuration/#cors-settings","title":"CORS Settings","text":"<p>Configure allowed origins in <code>settings.py</code>:</p> <pre><code>CORS_ALLOWED_ORIGINS = [\n    \"http://localhost:1420\",  # Vite dev server\n    \"tauri://localhost\",       # Tauri window\n]\n</code></pre>"},{"location":"getting-started/configuration/#tauri-configuration","title":"Tauri Configuration","text":"<p>Located in <code>client/src-tauri/tauri.conf.json</code>:</p>"},{"location":"getting-started/configuration/#window-settings","title":"Window Settings","text":"<pre><code>{\n  \"windows\": [\n    {\n      \"title\": \"AgentX\",\n      \"width\": 800,\n      \"height\": 600,\n      \"resizable\": true,\n      \"fullscreen\": false\n    }\n  ]\n}\n</code></pre>"},{"location":"getting-started/configuration/#development-url","title":"Development URL","text":"<pre><code>{\n  \"build\": {\n    \"devUrl\": \"http://localhost:1420\"\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#translation-models","title":"Translation Models","text":"<p>Configure models in <code>api/agentx_ai/kit/translation.py</code>:</p>"},{"location":"getting-started/configuration/#language-detection-model","title":"Language Detection Model","text":"<pre><code>DETECTION_MODEL = \"eleldar/language-detection\"\n</code></pre>"},{"location":"getting-started/configuration/#translation-model","title":"Translation Model","text":"<p>Currently using M2M100:</p> <pre><code>TRANSLATION_MODEL = \"facebook/m2m100_418M\"\n</code></pre> <p>To switch to NLLB-200 (larger, more accurate):</p> <pre><code>TRANSLATION_MODEL = \"facebook/nllb-200-distilled-600M\"\n</code></pre> <p>Note</p> <p>Larger models provide better quality but require more memory and slower inference.</p>"},{"location":"getting-started/configuration/#development-tools","title":"Development Tools","text":""},{"location":"getting-started/configuration/#task-configuration","title":"Task Configuration","text":"<p>Edit <code>Taskfile.yaml</code> to customize commands:</p> <pre><code>tasks:\n  api:runserver:\n    dir: api/\n    cmds:\n      - uv run python manage.py runserver 127.0.0.1:12319\n</code></pre>"},{"location":"getting-started/configuration/#vite-configuration","title":"Vite Configuration","text":"<p>Located in <code>client/vite.config.ts</code>:</p> <pre><code>export default defineConfig({\n  server: {\n    port: 1420,\n    strictPort: true,\n  },\n  // ... other settings\n})\n</code></pre>"},{"location":"getting-started/configuration/#security-considerations","title":"Security Considerations","text":""},{"location":"getting-started/configuration/#production-checklist","title":"Production Checklist","text":"<ul> <li> Change default database passwords</li> <li> Set <code>DJANGO_DEBUG=False</code></li> <li> Configure <code>DJANGO_ALLOWED_HOSTS</code></li> <li> Use environment variables for secrets</li> <li> Enable HTTPS for API</li> <li> Configure firewall rules</li> <li> Set up database backups</li> <li> Review CORS settings</li> </ul>"},{"location":"getting-started/configuration/#password-management","title":"Password Management","text":"<p>Never commit passwords to version control. Use:</p> <ul> <li>Environment variables</li> <li>Secret management tools (HashiCorp Vault, AWS Secrets Manager)</li> <li><code>.env</code> files (gitignored)</li> </ul>"},{"location":"getting-started/configuration/#performance-tuning","title":"Performance Tuning","text":""},{"location":"getting-started/configuration/#model-loading","title":"Model Loading","text":"<p>Translation models are loaded at startup. To reduce memory:</p> <pre><code># Use smaller models\nTRANSLATION_MODEL = \"facebook/m2m100_418M\"  # ~500MB\n# vs\nTRANSLATION_MODEL = \"facebook/nllb-200-3.3B\"  # ~13GB\n</code></pre>"},{"location":"getting-started/configuration/#database-connections","title":"Database Connections","text":"<p>Adjust connection pools in production:</p> <pre><code>DATABASES = {\n    'default': {\n        # ...\n        'OPTIONS': {\n            'MAX_CONNS': 20,\n            'MIN_CONNS': 5,\n        }\n    }\n}\n</code></pre>"},{"location":"getting-started/configuration/#redis-caching","title":"Redis Caching","text":"<p>Configure Redis for optimal performance:</p> <pre><code># In docker-compose.yml\ncommand: redis-server --maxmemory 512mb --maxmemory-policy allkeys-lru\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Development Setup - Advanced configuration</li> <li>Database Migration - Data management</li> <li>Task Commands - Available automation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Get AgentX up and running on your local machine.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing AgentX, ensure you have:</p> <ul> <li>Python 3.10+ - For the Django API</li> <li>Node.js 18+ - For the Tauri client</li> <li>Docker &amp; Docker Compose - For database services</li> <li>uv - Python package manager (installation guide)</li> <li>bun - Fast JavaScript runtime (installation guide)</li> <li>Task - Task runner (installation guide)</li> </ul>"},{"location":"getting-started/installation/#quick-install","title":"Quick Install","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/QR-Madness/agentx.git\ncd agentx\n</code></pre>"},{"location":"getting-started/installation/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code>task install\n</code></pre> <p>This command will:</p> <ul> <li>Install Python dependencies via <code>uv sync</code></li> <li>Install client dependencies via <code>bun install</code></li> <li>Install concurrently for running multiple services</li> </ul>"},{"location":"getting-started/installation/#3-initialize-databases","title":"3. Initialize Databases","text":"<pre><code>task db:init\n</code></pre> <p>This creates the required data directories for:</p> <ul> <li>Neo4j graph database</li> <li>PostgreSQL with pgvector</li> <li>Redis cache</li> </ul>"},{"location":"getting-started/installation/#4-start-development-environment","title":"4. Start Development Environment","text":"<pre><code>task dev\n</code></pre> <p>This will:</p> <ol> <li>Start Docker containers (Neo4j, Postgres, Redis)</li> <li>Launch Django API on <code>http://localhost:12319</code></li> <li>Launch Tauri development window</li> </ol>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":""},{"location":"getting-started/installation/#check-api","title":"Check API","text":"<pre><code>curl http://localhost:12319/api/index\n</code></pre> <p>Expected response: <pre><code>{\n  \"status\": \"ok\",\n  \"message\": \"AgentX API is running\"\n}\n</code></pre></p>"},{"location":"getting-started/installation/#check-databases","title":"Check Databases","text":"<pre><code># Neo4j Browser\nopen http://localhost:7474\n\n# Redis Commander (optional GUI)\nopen http://localhost:8081\n</code></pre> <p>Default credentials (from <code>.env.example</code>):</p> <ul> <li>Neo4j: <code>neo4j</code> / <code>changeme</code></li> <li>PostgreSQL: <code>agent</code> / <code>changeme</code></li> </ul> <p>Change default passwords</p> <p>Update passwords in your <code>.env</code> file before deploying to production.</p>"},{"location":"getting-started/installation/#manual-installation","title":"Manual Installation","text":"<p>If you prefer to install components individually:</p>"},{"location":"getting-started/installation/#python-api","title":"Python API","text":"<pre><code># Install dependencies\nuv sync\n\n# Run migrations\ntask api:migrate\n\n# Start API server\ntask api:runserver\n</code></pre>"},{"location":"getting-started/installation/#tauri-client","title":"Tauri Client","text":"<pre><code>cd client\n\n# Install dependencies\nbun install\n\n# Start development server\nbunx tauri dev\n</code></pre>"},{"location":"getting-started/installation/#database-services","title":"Database Services","text":"<pre><code># Start all databases\ntask runners\n\n# Or start individually\ndocker-compose up -d neo4j\ndocker-compose up -d postgres\ndocker-compose up -d redis\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#port-conflicts","title":"Port Conflicts","text":"<p>If ports are already in use, modify <code>docker-compose.yml</code>:</p> <ul> <li>Neo4j: 7474 (browser), 7687 (bolt)</li> <li>PostgreSQL: 5432</li> <li>Redis: 6379</li> <li>Django API: 12319</li> <li>Vite dev server: 1420</li> </ul>"},{"location":"getting-started/installation/#missing-data-directories","title":"Missing Data Directories","text":"<p>If you see errors about missing data directories:</p> <pre><code>task pre-launch-check\n</code></pre> <p>This will show which directories are missing and how to fix them.</p>"},{"location":"getting-started/installation/#docker-volume-migration","title":"Docker Volume Migration","text":"<p>If you have existing data in Docker volumes:</p> <pre><code>task db:migrate-volumes\n</code></pre> <p>This migrates data from Docker volumes to local bind mounts in <code>./data/</code>.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Learn basic usage</li> <li>Configuration - Customize your setup</li> <li>Development Setup - Set up for development</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get started with AgentX in 5 minutes.</p>"},{"location":"getting-started/quickstart/#starting-agentx","title":"Starting AgentX","text":""},{"location":"getting-started/quickstart/#development-mode","title":"Development Mode","text":"<pre><code>task dev\n</code></pre> <p>This starts:</p> <ul> <li>All database services (Neo4j, Postgres, Redis)</li> <li>Django API on port 12319</li> <li>Tauri desktop application</li> </ul>"},{"location":"getting-started/quickstart/#api-only","title":"API Only","text":"<pre><code>task api:runserver\n</code></pre>"},{"location":"getting-started/quickstart/#client-only","title":"Client Only","text":"<pre><code>cd client\nbunx tauri dev\n</code></pre>"},{"location":"getting-started/quickstart/#using-the-translation-api","title":"Using the Translation API","text":""},{"location":"getting-started/quickstart/#detect-language","title":"Detect Language","text":"<pre><code>curl http://localhost:12319/api/language-detect\n</code></pre>"},{"location":"getting-started/quickstart/#translate-text","title":"Translate Text","text":"<pre><code>curl -X POST http://localhost:12319/api/translate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"text\": \"Hello, world!\",\n    \"target_language\": \"fr\"\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"original\": \"Hello, world!\",\n  \"translated\": \"Bonjour le monde!\",\n  \"source_language\": \"en\",\n  \"target_language\": \"fr\",\n  \"confidence\": 0.98\n}\n</code></pre></p>"},{"location":"getting-started/quickstart/#supported-language-codes","title":"Supported Language Codes","text":"<p>Use ISO 639-1 codes for target languages:</p> <ul> <li><code>en</code> - English</li> <li><code>fr</code> - French</li> <li><code>es</code> - Spanish</li> <li><code>de</code> - German</li> <li><code>zh</code> - Chinese</li> <li><code>ja</code> - Japanese</li> <li><code>ar</code> - Arabic</li> <li>And 193+ more...</li> </ul>"},{"location":"getting-started/quickstart/#using-the-desktop-application","title":"Using the Desktop Application","text":""},{"location":"getting-started/quickstart/#tab-navigation","title":"Tab Navigation","text":"<p>The application has four main tabs:</p> <ol> <li>Dashboard - Overview and stats</li> <li>Translation - Interactive translation interface</li> <li>Chat - AI conversation interface</li> <li>Tools - Utilities and settings</li> </ol>"},{"location":"getting-started/quickstart/#translation-tab","title":"Translation Tab","text":"<ol> <li>Enter text in the source field</li> <li>Select target language</li> <li>Click \"Translate\"</li> <li>View results with confidence scores</li> </ol>"},{"location":"getting-started/quickstart/#database-access","title":"Database Access","text":""},{"location":"getting-started/quickstart/#postgresql-shell","title":"PostgreSQL Shell","text":"<pre><code>task db:shell:postgres\n</code></pre>"},{"location":"getting-started/quickstart/#redis-cli","title":"Redis CLI","text":"<pre><code>task db:shell:redis\n</code></pre>"},{"location":"getting-started/quickstart/#neo4j-browser","title":"Neo4j Browser","text":"<p>Open http://localhost:7474</p> <p>Credentials: <code>neo4j</code> / <code>your_secure_password</code></p>"},{"location":"getting-started/quickstart/#common-tasks","title":"Common Tasks","text":""},{"location":"getting-started/quickstart/#run-tests","title":"Run Tests","text":"<pre><code>task test\n</code></pre>"},{"location":"getting-started/quickstart/#database-backup","title":"Database Backup","text":"<pre><code>task db:backup:postgres\n</code></pre> <p>Backups are saved in <code>./backups/</code></p>"},{"location":"getting-started/quickstart/#clean-database","title":"Clean Database","text":"<pre><code>task db:clean\n</code></pre> <p>Warning</p> <p>This removes all database data. Use with caution!</p>"},{"location":"getting-started/quickstart/#stop-services","title":"Stop Services","text":"<pre><code>task teardown\n</code></pre> <p>Or press <code>Ctrl+C</code> to stop the dev environment.</p>"},{"location":"getting-started/quickstart/#development-workflow","title":"Development Workflow","text":"<ol> <li>Start dev environment: <code>task dev</code></li> <li>Make changes to API or client code</li> <li>Hot reload happens automatically</li> <li>Vite for frontend changes</li> <li>Django autoreload for API changes</li> <li>Run tests: <code>task test</code></li> <li>Commit changes: <code>git commit</code></li> <li>Stop services: <code>Ctrl+C</code> or <code>task teardown</code></li> </ol>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Customize settings</li> <li>Architecture Overview - Understand the system</li> <li>Development Setup - Advanced development</li> </ul>"}]}