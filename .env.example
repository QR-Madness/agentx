# AgentX Environment Configuration
# Copy this file to .env and fill in your values
#
# AgentX is OFFLINE-FIRST: Local Ollama models are the default.
# Cloud providers (OpenAI, Anthropic) are optional for offloading.

# =============================================================================
# Default Model Settings (Offline-First)
# =============================================================================

# Default model for reasoning/chat (uses Ollama by default)
# Examples: llama3.2, qwen2.5, mistral, mixtral, codellama
DEFAULT_MODEL=llama3.2

# =============================================================================
# Local Model Settings (Primary - Offline-First)
# =============================================================================

# LM Studio server URL (preferred - OpenAI-compatible API)
# LM Studio provides a simple local server with GUI model management
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_TIMEOUT=600

# Ollama server URL (alternative to LM Studio)
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_TIMEOUT=600

# HuggingFace token (optional, for gated models)
HF_TOKEN=

# =============================================================================
# Cloud Providers (Optional - for offloading)
# =============================================================================

# OpenAI (for GPT-4, GPT-3.5, embeddings)
OPENAI_API_KEY=

# Anthropic (for Claude models)
ANTHROPIC_API_KEY=

# Together.ai (optional, for hosted open models)
TOGETHER_API_KEY=

# =============================================================================
# Database Credentials
# =============================================================================

# Neo4j Graph Database
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=changeme

# PostgreSQL (with pgvector)
POSTGRES_USER=agent
POSTGRES_PASSWORD=changeme
POSTGRES_DB=agent_memory
POSTGRES_URI=postgresql://agent:changeme@localhost:5432/agent_memory

# Redis Cache
REDIS_URI=redis://localhost:6379

# =============================================================================
# Embedding Configuration
# =============================================================================

# Embedding provider: "openai" or "local"
EMBEDDING_PROVIDER=local

# OpenAI embedding model (if using OpenAI)
EMBEDDING_MODEL=text-embedding-3-small

# Local embedding model (if using local)
LOCAL_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5

# =============================================================================
# Application Settings
# =============================================================================

# Django secret key (generate a new one for production)
# Use: python -c "from django.core.management.utils import get_random_secret_key; print(get_random_secret_key())"
DJANGO_SECRET_KEY=your-secret-key-here

# Debug mode (set to false in production)
DJANGO_DEBUG=true

# Allowed hosts (comma-separated, defaults to localhost,127.0.0.1)
DJANGO_ALLOWED_HOSTS=localhost,127.0.0.1

# API server port
API_PORT=12319

# =============================================================================
# Client Settings
# =============================================================================

# API URL for the Tauri client (Vite env var)
VITE_API_URL=http://localhost:12319/api
