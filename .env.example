# AgentX Environment Configuration
# Copy this file to .env and fill in your values
#
# AgentX supports local models (LM Studio) and cloud providers (Anthropic, OpenAI).
# Configure via Settings UI or environment variables below.

# =============================================================================
# Default Model Settings
# =============================================================================

# Default model for reasoning/chat
# Examples: llama3.2, qwen2.5, mistral, claude-3-5-sonnet-latest
DEFAULT_MODEL=llama3.2

# =============================================================================
# LM Studio (Local Models)
# =============================================================================

# LM Studio server URL (OpenAI-compatible API)
# Can also be configured via Settings UI in the app
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_TIMEOUT=600

# HuggingFace token (optional, for gated models)
HF_TOKEN=

# =============================================================================
# Cloud Providers
# =============================================================================

# Anthropic (primary cloud provider - Claude models)
# Can also be configured via Settings UI in the app
ANTHROPIC_API_KEY=

# OpenAI (experimental - GPT models)
# Can also be configured via Settings UI in the app
OPENAI_API_KEY=

# Together.ai (optional, for hosted open models)
TOGETHER_API_KEY=

# =============================================================================
# Database Credentials
# =============================================================================

# Neo4j Graph Database
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=changeme

# PostgreSQL (with pgvector)
POSTGRES_USER=agent
POSTGRES_PASSWORD=changeme
POSTGRES_DB=agent_memory
POSTGRES_URI=postgresql://agent:changeme@localhost:5432/agent_memory

# Redis Cache
REDIS_URI=redis://localhost:6379

# =============================================================================
# Embedding Configuration
# =============================================================================

# Embedding provider: "openai" or "local"
EMBEDDING_PROVIDER=local

# OpenAI embedding model (if using OpenAI)
EMBEDDING_MODEL=text-embedding-3-small

# Local embedding model (if using local)
LOCAL_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5

# =============================================================================
# Application Settings
# =============================================================================

# Django secret key (generate a new one for production)
# Use: python -c "from django.core.management.utils import get_random_secret_key; print(get_random_secret_key())"
DJANGO_SECRET_KEY=your-secret-key-here

# Debug mode (set to false in production)
DJANGO_DEBUG=true

# Allowed hosts (comma-separated, defaults to localhost,127.0.0.1)
DJANGO_ALLOWED_HOSTS=localhost,127.0.0.1

# API server port
API_PORT=12319

# =============================================================================
# Client Settings
# =============================================================================

# API URL for the Tauri client (Vite env var)
VITE_API_URL=http://localhost:12319/api

# =============================================================================
# Security Settings (Foundation - currently permissive)
# =============================================================================

# CORS allowed origins (comma-separated, defaults to localhost:1420)
# Set to specific origins in production
# CORS_ALLOWED_ORIGINS=http://localhost:1420,tauri://localhost

# CSRF trusted origins (comma-separated)
# CSRF_TRUSTED_ORIGINS=http://localhost:1420,tauri://localhost

# Input size limits
AGENTX_MAX_TEXT_LENGTH=100000
AGENTX_MAX_CHAT_LENGTH=10000

# Rate limiting (not enforced by default)
AGENTX_RATE_LIMIT_ENABLED=false
AGENTX_RATE_LIMIT_DEFAULT=100/m

# API key authentication (not enforced by default)
# When enabled, non-localhost requests require X-API-Key header
AGENTX_API_KEY_REQUIRED=false
AGENTX_API_KEY=
