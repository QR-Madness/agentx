"""
Speculative Decoding implementation.

Speculative decoding uses a fast draft model to generate tokens quickly,
then a more accurate target model to verify/correct them in parallel.
This can significantly speed up generation while maintaining quality.
"""

import logging
import time
from dataclasses import dataclass
from typing import Any

from ..providers.base import Message
from ..providers.registry import get_registry
from .base import DraftingConfig, DraftingStrategy, DraftResult, DraftStatus

logger = logging.getLogger(__name__)


@dataclass
class SpeculativeConfig:
    """Configuration for speculative decoding."""
    draft_model: str  # Fast model for drafting
    target_model: str  # Accurate model for verification
    draft_tokens: int = 20  # Tokens to draft before verification
    acceptance_threshold: float = 0.8  # Min similarity for acceptance
    max_iterations: int = 10  # Max draft/verify cycles
    parallel_verification: bool = True  # Verify while drafting next


class SpeculativeDecoder(DraftingStrategy):
    """
    Speculative decoding strategy.
    
    Uses a smaller/faster draft model to generate candidate tokens,
    then a larger/more accurate target model to verify them.
    Accepted tokens are kept, rejected tokens are regenerated by target.
    
    Example usage:
        decoder = SpeculativeDecoder(SpeculativeConfig(
            draft_model="gpt-3.5-turbo",
            target_model="gpt-4-turbo",
            draft_tokens=20,
        ))
        result = await decoder.generate(messages)
    """
    
    def __init__(self, config: SpeculativeConfig):
        self.spec_config = config
        super().__init__(DraftingConfig(
            name=f"speculative-{config.draft_model}->{config.target_model}",
            strategy_type="speculative",
            extra={"spec_config": config},
        ))
        self._registry = None
    
    @property
    def name(self) -> str:
        return self.config.name
    
    @property
    def strategy_type(self) -> str:
        return "speculative"
    
    @property
    def registry(self):
        """Lazy-load the provider registry."""
        if self._registry is None:
            self._registry = get_registry()
        return self._registry
    
    async def generate(
        self,
        messages: list[Message],
        **kwargs: Any,
    ) -> DraftResult:
        """
        Generate content using speculative decoding.
        
        The process:
        1. Draft model generates N tokens quickly
        2. Target model verifies the draft
        3. Accept matching tokens, regenerate rejected ones
        4. Repeat until complete or max iterations
        """
        start_time = time.time()
        
        draft_provider, draft_model_id = self.registry.get_provider_for_model(
            self.spec_config.draft_model
        )
        target_provider, target_model_id = self.registry.get_provider_for_model(
            self.spec_config.target_model
        )
        
        logger.info(
            f"Speculative decoding: {self.spec_config.draft_model} -> {self.spec_config.target_model}"
        )
        
        total_draft_tokens = 0
        total_accepted_tokens = 0
        total_input_tokens = 0
        total_output_tokens = 0
        draft_time = 0.0
        verify_time = 0.0
        
        # Build up the response
        generated_content = ""
        current_messages = list(messages)
        
        for iteration in range(self.spec_config.max_iterations):
            # Step 1: Draft phase
            draft_start = time.time()
            
            try:
                draft_result = await draft_provider.complete(
                    current_messages,
                    draft_model_id,
                    temperature=kwargs.get("temperature", 0.7),
                    max_tokens=self.spec_config.draft_tokens,
                )
            except Exception as e:
                logger.error(f"Draft generation failed: {e}")
                return DraftResult(
                    content=generated_content,
                    strategy=self.name,
                    status=DraftStatus.FAILED,
                    models_used=[self.spec_config.draft_model, self.spec_config.target_model],
                )
            
            draft_time += (time.time() - draft_start) * 1000
            draft_content = draft_result.content
            total_draft_tokens += len(draft_content.split())  # Approximate
            
            if draft_result.usage:
                total_input_tokens += draft_result.usage.get("prompt_tokens", 0)
                total_output_tokens += draft_result.usage.get("completion_tokens", 0)
            
            # Check if draft indicates completion
            if draft_result.finish_reason in ("stop", "end_turn"):
                generated_content += draft_content
                total_accepted_tokens += len(draft_content.split())
                logger.debug(f"Draft complete at iteration {iteration + 1}")
                break
            
            # Step 2: Verification phase
            verify_start = time.time()
            
            # Create verification prompt
            verify_messages = current_messages + [
                Message(role="assistant", content=draft_content),
            ]
            
            try:
                verify_result = await target_provider.complete(
                    verify_messages,
                    target_model_id,
                    temperature=kwargs.get("temperature", 0.7),
                    max_tokens=self.spec_config.draft_tokens + 10,  # Allow slight expansion
                )
            except Exception as e:
                logger.error(f"Verification failed: {e}")
                # Fall back to draft content
                generated_content += draft_content
                total_accepted_tokens += len(draft_content.split())
                break
            
            verify_time += (time.time() - verify_start) * 1000
            
            if verify_result.usage:
                total_input_tokens += verify_result.usage.get("prompt_tokens", 0)
                total_output_tokens += verify_result.usage.get("completion_tokens", 0)
            
            # Step 3: Accept/reject logic
            # Simple approach: accept the target model's version
            # More sophisticated: token-by-token comparison
            verified_content = verify_result.content
            
            # Calculate acceptance ratio (approximate)
            draft_words = set(draft_content.lower().split())
            verified_words = set(verified_content.lower().split())
            if draft_words:
                overlap = len(draft_words & verified_words) / len(draft_words)
            else:
                overlap = 0.0
            
            if overlap >= self.spec_config.acceptance_threshold:
                # High overlap - accept draft
                generated_content += draft_content
                total_accepted_tokens += len(draft_content.split())
                logger.debug(f"Iteration {iteration + 1}: Accepted draft (overlap={overlap:.2f})")
            else:
                # Low overlap - use target's version
                generated_content += verified_content
                total_accepted_tokens += int(len(verified_content.split()) * overlap)
                logger.debug(f"Iteration {iteration + 1}: Used target (overlap={overlap:.2f})")
            
            # Update messages for next iteration
            current_messages = messages + [
                Message(role="assistant", content=generated_content),
            ]
            
            # Check if complete
            if verify_result.finish_reason in ("stop", "end_turn"):
                break
        
        total_time = (time.time() - start_time) * 1000
        
        # Estimate cost
        draft_caps = draft_provider.get_capabilities(draft_model_id)
        target_caps = target_provider.get_capabilities(target_model_id)
        
        estimated_cost = 0.0
        if draft_caps.cost_per_1k_input and draft_caps.cost_per_1k_output:
            estimated_cost += (total_input_tokens / 1000) * draft_caps.cost_per_1k_input * 0.5
            estimated_cost += (total_output_tokens / 1000) * draft_caps.cost_per_1k_output * 0.5
        if target_caps.cost_per_1k_input and target_caps.cost_per_1k_output:
            estimated_cost += (total_input_tokens / 1000) * target_caps.cost_per_1k_input * 0.5
            estimated_cost += (total_output_tokens / 1000) * target_caps.cost_per_1k_output * 0.5
        
        return DraftResult(
            content=generated_content,
            strategy=self.name,
            status=DraftStatus.COMPLETE,
            draft_tokens=total_draft_tokens,
            accepted_tokens=total_accepted_tokens,
            total_tokens=total_draft_tokens,
            draft_time_ms=draft_time,
            verify_time_ms=verify_time,
            total_time_ms=total_time,
            models_used=[self.spec_config.draft_model, self.spec_config.target_model],
            stages_completed=iteration + 1,
            input_tokens=total_input_tokens,
            output_tokens=total_output_tokens,
            estimated_cost=estimated_cost,
        )
    
    async def validate(self) -> bool:
        """Validate that both models are accessible."""
        try:
            self.registry.get_provider_for_model(self.spec_config.draft_model)
            self.registry.get_provider_for_model(self.spec_config.target_model)
            return True
        except ValueError as e:
            logger.error(f"Validation failed: {e}")
            return False
    
    def get_description(self) -> str:
        return (
            f"Speculative decoding: {self.spec_config.draft_model} drafts "
            f"{self.spec_config.draft_tokens} tokens, "
            f"{self.spec_config.target_model} verifies"
        )
